{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/Murrell2025/blob/main/Murrell_2026_Fig2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Murrell 2026 Figure 2\n",
        "<br>\n",
        "<img src=\"https://fed3bandit.readthedocs.io/en/latest/_static/fed3bandit_logo1.svg\" width=\"200\" />\n",
        "\n",
        "Authors: Chantelle Murrell<br>\n",
        "Updated: 12-30-25  "
      ],
      "metadata": {
        "id": "ru9_Mj2OpjhT"
      },
      "id": "ru9_Mj2OpjhT"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install libraries and import them {\"run\":\"auto\"}\n",
        "\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Packages to ensure are installed (add others here if you like)\n",
        "packages = {\n",
        "    \"fed3\": \"git+https://github.com/earnestt1234/fed3.git\",\n",
        "    \"fed3bandit\": \"fed3bandit\",\n",
        "    \"pingouin\": \"pingouin\",\n",
        "    \"ipydatagrid\": \"ipydatagrid\",\n",
        "    \"openpyxl\": \"openpyxl\",\n",
        "}\n",
        "\n",
        "for name, source in packages.items():\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        print(f\"Installing {name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", source])\n",
        "\n",
        "# ----------------------------\n",
        "# Imports\n",
        "# ----------------------------\n",
        "# Standard library\n",
        "import copy\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import threading\n",
        "import time\n",
        "import warnings\n",
        "import zipfile\n",
        "import requests\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "from os.path import basename, splitext\n",
        "\n",
        "# Third-party\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "import fed3\n",
        "import fed3.plot as fplot\n",
        "import fed3bandit as f3b\n",
        "from scipy.stats import f_oneway\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "from google.colab import files\n",
        "try:\n",
        "    from tqdm.auto import tqdm   # nice in notebooks; falls back to std tqdm on console\n",
        "except Exception:\n",
        "    # safe no-op fallback if tqdm isn't installed\n",
        "    def tqdm(x):\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams.update({'font.size': 12, 'figure.autolayout': True})\n",
        "plt.rcParams['figure.figsize'] = [6, 4]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "print(\"Packages installed and imports ready.\")\n"
      ],
      "metadata": {
        "id": "c_vNg7dvVfCh",
        "cellView": "form",
        "collapsed": true
      },
      "id": "c_vNg7dvVfCh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import FED3 FR1 Data\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "import os, zipfile, shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from google.colab import output as colab_output\n",
        "    colab_output.enable_custom_widget_manager()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "zip_url = \"https://github.com/KravitzLab/Murrell2025/raw/refs/heads/main/Data/FR1.zip\"\n",
        "key_url = \"https://github.com/KravitzLab/Murrell2025/raw/refs/heads/main/Data/Murrell2026_Key.csv\"\n",
        "\n",
        "zip_dir = \"/content/Murrell2025_zipdata\"\n",
        "zip_path = os.path.join(zip_dir, \"Bandit100.zip\")\n",
        "extract_root = os.path.join(zip_dir, \"Bandit100_extracted\")\n",
        "key_path = os.path.join(zip_dir, \"Murrell2026_Key.csv\")\n",
        "\n",
        "os.makedirs(zip_dir, exist_ok=True)\n",
        "\n",
        "# download + unzip (fresh each run)\n",
        "if os.path.exists(zip_path):\n",
        "    os.remove(zip_path)\n",
        "if os.path.isdir(extract_root):\n",
        "    shutil.rmtree(extract_root)\n",
        "\n",
        "print(\"Importing github.com/KravitzLab/Murrell2025/Data/FR1.zip ...\")\n",
        "urlretrieve(zip_url, zip_path)\n",
        "\n",
        "os.makedirs(extract_root, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(extract_root)\n",
        "\n",
        "# if the zip contains a Bandit100 folder, use it; otherwise use extract_root\n",
        "bandit_root = os.path.join(extract_root, \"Bandit100\")\n",
        "local_parent_path = bandit_root if os.path.isdir(bandit_root) else extract_root\n",
        "\n",
        "# load CSVs\n",
        "feds, loaded_files, session_types = [], [], []\n",
        "\n",
        "for dirpath, _, filenames in os.walk(local_parent_path):\n",
        "    for file_name in sorted(filenames):\n",
        "        if file_name.lower().endswith(\".csv\"):\n",
        "            file_path = os.path.join(dirpath, file_name)\n",
        "            strain_name = os.path.basename(os.path.dirname(file_path))\n",
        "\n",
        "            df = fed3.load(file_path)\n",
        "            df.name = file_name\n",
        "            df[\"Strain\"] = strain_name\n",
        "            df[\"SourceFile\"] = file_name\n",
        "\n",
        "            feds.append(df)\n",
        "            loaded_files.append(file_path)\n",
        "\n",
        "            st = df[\"Session_Type\"].dropna().astype(str)\n",
        "            session_types.append(st.iloc[0] if len(st) else None)\n",
        "\n",
        "print(f\"Loaded {len(feds)} CSV files.\")\n",
        "\n",
        "# download + load key\n",
        "urlretrieve(key_url, key_path)\n",
        "\n",
        "key_df = pd.read_csv(key_path, encoding=\"utf-8-sig\")\n",
        "key_df[\"Mouse_ID\"] = key_df[\"Mouse_ID\"].astype(str).str.strip()\n",
        "\n",
        "# match Mouse_ID by substring in filename base\n",
        "def _base_lower(p):\n",
        "    return os.path.splitext(os.path.basename(p))[0].lower()\n",
        "\n",
        "files_df = pd.DataFrame({\"filename\": loaded_files, \"Session_type\": session_types})\n",
        "files_df[\"_base\"] = files_df[\"filename\"].map(_base_lower)\n",
        "\n",
        "mouse_ids = (\n",
        "    key_df[\"Mouse_ID\"]\n",
        "    .dropna().astype(str).str.strip()\n",
        "    .replace(\"\", np.nan).dropna().unique().tolist()\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for fname, base in zip(files_df[\"filename\"], files_df[\"_base\"]):\n",
        "    hits = [mid for mid in mouse_ids if mid.lower() in base]\n",
        "    rows.append({\"filename\": fname, \"Mouse_ID\": hits[0] if len(hits) else None})\n",
        "\n",
        "matched = pd.DataFrame(rows)\n",
        "\n",
        "Key_Df = (\n",
        "    files_df.drop(columns=[\"_base\"])\n",
        "    .merge(matched, on=\"filename\", how=\"left\")\n",
        "    .merge(key_df.drop_duplicates(\"Mouse_ID\"), on=\"Mouse_ID\", how=\"left\")\n",
        ")\n",
        "\n",
        "# display\n",
        "grid = DataGrid(\n",
        "    Key_Df.reset_index(drop=True),\n",
        "    editable=True,\n",
        "    selection_mode=\"cell\",\n",
        "    layout={\"height\": \"420px\"},\n",
        "    base_row_size=28,\n",
        "    base_column_size=120,\n",
        ")\n",
        "grid.default_renderer = TextRenderer(text_wrap=True)\n",
        "display(grid)"
      ],
      "metadata": {
        "id": "_3hqwt-oejwc",
        "cellView": "form"
      },
      "id": "_3hqwt-oejwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot individual files (males are blue, females are red)\n",
        "# ----- Inputs -----\n",
        "assert 'feds' in globals() and isinstance(feds, list) and len(feds) > 0, \"No FED3 files loaded.\"\n",
        "assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "\n",
        "TIMESTAMP_COL_CANON = \"MM:DD:YYYY hh:mm:ss\"  # primary target column name\n",
        "\n",
        "# ----- Helpers -----\n",
        "def _find_time_col(df):\n",
        "    # exact match first\n",
        "    if TIMESTAMP_COL_CANON in df.columns:\n",
        "        return TIMESTAMP_COL_CANON\n",
        "    # tolerant search (case/space-insensitive)\n",
        "    lc = {str(c).strip().lower(): c for c in df.columns}\n",
        "    for key in lc:\n",
        "        if key.replace(\" \", \"\") in {\"mm:dd:yyyyhh:mm:ss\", \"mm:dd:yyyy_hh:mm:ss\", \"mm/dd/yyyyhh:mm:ss\"}:\n",
        "            return lc[key]\n",
        "    return None\n",
        "\n",
        "def _parse_ts(series):\n",
        "    # robust parsing; coerce errors to NaT\n",
        "    return pd.to_datetime(series, errors=\"coerce\", infer_datetime_format=True)\n",
        "\n",
        "\n",
        "# ----- Plotting\n",
        "files_list = feds\n",
        "\n",
        "# metadata_df = copy of Key_Df\n",
        "metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "if 'filename' in metadata_df.columns:\n",
        "    metadata_df['filename'] = metadata_df['filename'].astype(str).map(os.path.basename)\n",
        "\n",
        "def _coerce_numeric_col(df, col, clip_upper=None, na_map=None):\n",
        "    if col not in df.columns:\n",
        "        return\n",
        "    s = df[col]\n",
        "    if na_map:\n",
        "        s = s.replace(na_map)\n",
        "    s = pd.to_numeric(s, errors='coerce')\n",
        "    if clip_upper is not None:\n",
        "        s.loc[s > clip_upper] = np.nan\n",
        "    df[col] = s\n",
        "\n",
        "def _plot_file_core(file_index):\n",
        "    df = files_list[file_index].copy()\n",
        "    full_name = getattr(df, 'name', f\"File_{file_index}\")\n",
        "    file_basename = os.path.basename(str(full_name))\n",
        "\n",
        "    # Preserve original index once\n",
        "    if \"Original_Timestamp\" not in df.columns:\n",
        "        df[\"Original_Timestamp\"] = df.index\n",
        "\n",
        "    # Attach metadata by filename (matching already done upstream)\n",
        "    meta_row = None\n",
        "    if 'filename' in metadata_df.columns:\n",
        "        mr = metadata_df.loc[metadata_df['filename'] == file_basename]\n",
        "        if not mr.empty:\n",
        "            meta_row = mr.iloc[0]\n",
        "    if meta_row is not None:\n",
        "        for col in meta_row.index:\n",
        "            if col == 'filename':\n",
        "                continue\n",
        "            if col not in df.columns:\n",
        "                df[col] = meta_row[col]\n",
        "            else:\n",
        "                if pd.isna(df[col]).all() and pd.notna(meta_row[col]):\n",
        "                    df[col] = meta_row[col]\n",
        "\n",
        "    # Time + cleanup\n",
        "    try:\n",
        "        df['timestamp'] = pd.to_datetime(df.index)\n",
        "    except Exception:\n",
        "        df['timestamp'] = np.arange(len(df))\n",
        "\n",
        "    _coerce_numeric_col(df, 'Poke_Time', clip_upper=2)\n",
        "    _coerce_numeric_col(df, 'Retrieval_Time', na_map={\"Timed_out\": np.nan})\n",
        "\n",
        "    if len(df) == 0:\n",
        "        print(f\"[!] Empty cropped dataframe for {file_basename}. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    # Behavioral traces (needs f3b)\n",
        "    true_left = 1\n",
        "    mouse_left = f3b.binned_paction(df, window=10)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(3.3, 3))\n",
        "    ax.plot(1, 1, color=\"black\", linewidth=2, alpha=0.5)\n",
        "\n",
        "    color = \"dodgerblue\"\n",
        "    if 'Sex' in df.columns and pd.notna(df['Sex']).any():\n",
        "        try:\n",
        "            color = \"red\" if str(df['Sex'].iloc[0]).strip().lower().startswith(\"f\") else \"dodgerblue\"\n",
        "        except Exception:\n",
        "            pass\n",
        "    ax.plot(np.arange(len(mouse_left)), mouse_left, color=color, linewidth=3, alpha=0.7)\n",
        "\n",
        "    # ---- Clean look: remove ticks, labels, spines ----\n",
        "    ax.set_xlabel(\"\")                 # no x label\n",
        "    ax.set_ylabel(\"\")                 # no y label\n",
        "    ax.tick_params(axis=\"both\", which=\"both\",\n",
        "                   bottom=False, top=False, left=False, right=False,\n",
        "                   labelbottom=False, labelleft=False)\n",
        "    for s in ax.spines.values():      # remove axis lines\n",
        "        s.set_visible(False)\n",
        "\n",
        "    # ---- Textual y \"labels\" at y=1 and y=0 (not ticks) ----\n",
        "    ax.text(-0.01, 1.0, \"Left\", transform=ax.get_yaxis_transform(),\n",
        "            ha=\"right\", va=\"center\", fontsize = 12)\n",
        "    ax.text(-0.01, 0.0, \"Right\",  transform=ax.get_yaxis_transform(),\n",
        "            ha=\"right\", va=\"center\", fontsize = 12)\n",
        "\n",
        "    # ---- Title-area arrow above the axes ----\n",
        "    # spans full width; adjust y (1.08–1.15) if you need more/less space\n",
        "    ax.annotate(\"\",\n",
        "                xy=(0.95, 1.05), xytext=(0.05, 1.05),\n",
        "                xycoords=\"axes fraction\",\n",
        "                arrowprops=dict(arrowstyle=\"->\", lw=5, color=\"0.6\"))\n",
        "    # Optional: small caption above the arrow (example: duration text)\n",
        "    ax.text(0.4, 1.15, \"1 day\", transform=ax.transAxes, ha=\"left\", va=\"bottom\", color=\"0.5\", fontsize = 12)\n",
        "    sns.despine(left=True, bottom=True, top=True, right=True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ----- Simple UI: slider + status + output -----\n",
        "N = len(files_list)\n",
        "assert N > 0, \"No files available after cropping.\"\n",
        "idx_slider = widgets.IntSlider(min=0, max=max(0, N-1), step=1, value=0, description='File', continuous_update=True)\n",
        "status_lbl = widgets.HTML()\n",
        "out = widgets.Output()\n",
        "\n",
        "def _status(idx):\n",
        "    name = getattr(files_list[idx], 'name', f\"File_{idx}\")\n",
        "    return f\"Index: <b>{idx}</b> &nbsp;|&nbsp; File: <code>{os.path.basename(str(name))}</code> &nbsp;|&nbsp; Rows: {len(files_list[idx])}\"\n",
        "\n",
        "def _render(*_):\n",
        "    idx = int(idx_slider.value)\n",
        "    status_lbl.value = _status(idx)\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        _plot_file_core(idx)\n",
        "\n",
        "idx_slider.observe(_render, names='value')\n",
        "display(widgets.VBox([idx_slider, status_lbl, out]))\n",
        "_render()\n"
      ],
      "metadata": {
        "id": "qeZfA9tXWngI",
        "cellView": "form"
      },
      "id": "qeZfA9tXWngI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Analyze FR1 metrics\n",
        "from pathlib import Path\n",
        "import os\n",
        "def _find_time_col(df):\n",
        "    for c in [\"MM:DD:YYYY hh:mm:ss\", \"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _get_timestamp_series(df, ts_col=\"MM:DD:YYYY hh:mm:ss\"):\n",
        "    import pandas as pd\n",
        "    if ts_col in df.columns:\n",
        "        ts = pd.to_datetime(df[ts_col], format=\"%m:%d:%Y %H:%M:%S\", errors=\"coerce\")\n",
        "        return pd.Series(ts, index=df.index)\n",
        "    for cand in [\"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "        if cand in df.columns:\n",
        "            ts = pd.to_datetime(df[cand], errors=\"coerce\")\n",
        "            return pd.Series(ts, index=df.index)\n",
        "    idx = df.index\n",
        "    if isinstance(idx, pd.DatetimeIndex):\n",
        "        return pd.Series(idx, index=df.index)\n",
        "    return pd.to_datetime(pd.Series(idx, index=df.index), errors=\"coerce\")\n",
        "\n",
        "def _crop_last_24h(df):\n",
        "    import pandas as pd\n",
        "    dfc = df.copy()\n",
        "    ts_col = _find_time_col(dfc)\n",
        "\n",
        "    if ts_col is not None:\n",
        "        ts_series = pd.to_datetime(dfc[ts_col], errors=\"coerce\", infer_datetime_format=True)\n",
        "        ts_series = pd.Series(ts_series, index=dfc.index)\n",
        "    else:\n",
        "        try:\n",
        "            ts_idx = pd.to_datetime(dfc.index, errors=\"coerce\", infer_datetime_format=True)\n",
        "            ts_series = pd.Series(ts_idx, index=dfc.index)\n",
        "        except Exception:\n",
        "            return df  # no usable timestamps → return original\n",
        "\n",
        "    if ts_series.isna().all():\n",
        "        return df\n",
        "\n",
        "    end = ts_series.max()\n",
        "    if pd.isna(end):\n",
        "        return df\n",
        "    start = end - pd.Timedelta(hours=24)\n",
        "\n",
        "    mask = ts_series.between(start, end, inclusive=\"both\")\n",
        "    cropped = dfc.loc[mask]\n",
        "\n",
        "    if hasattr(df, \"name\"):\n",
        "        cropped.name = df.name\n",
        "    return cropped\n",
        "\n",
        "def build_feds_cropped(sessions):\n",
        "    \"\"\"Return a list of sessions cropped to their last 24h.\"\"\"\n",
        "    return [_crop_last_24h(d) for d in sessions]\n",
        "\n",
        "# Use it like this:\n",
        "assert 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0, \"No FED3 files available.\"\n",
        "feds_cropped = build_feds_cropped(feds)\n",
        "\n",
        "# Prefer cropped sessions downstream\n",
        "_sessions = list(feds_cropped) if len(feds_cropped) > 0 else list(feds)\n",
        "# ----- Build feds_cropped -----\n",
        "feds_cropped = [_crop_last_24h(d) for d in feds]\n",
        "# ---------- Inputs ----------\n",
        "# Prefer cropped sessions\n",
        "if 'feds_cropped' in globals() and isinstance(feds_cropped, (list, tuple)) and len(feds_cropped) > 0:\n",
        "    _sessions = list(feds_cropped)\n",
        "else:\n",
        "    assert 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0, \"No FED3 files available.\"\n",
        "    _sessions = list(feds)\n",
        "\n",
        "# metadata_df from Key_Df\n",
        "if 'metadata_df' not in globals() or not isinstance(metadata_df, pd.DataFrame):\n",
        "    assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "    metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "\n",
        "def _basename(pathlike) -> str:\n",
        "    s = str(pathlike).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "def _get_timestamp_series(df, ts_col=\"MM:DD:YYYY hh:mm:ss\"):\n",
        "    if ts_col in df.columns:\n",
        "        ts = pd.to_datetime(df[ts_col], format=\"%m:%d:%Y %H:%M:%S\", errors=\"coerce\")\n",
        "        return pd.Series(ts, index=df.index)\n",
        "    for cand in [\"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "        if cand in df.columns:\n",
        "            ts = pd.to_datetime(df[cand], errors=\"coerce\")\n",
        "            return pd.Series(ts, index=df.index)\n",
        "    idx = df.index\n",
        "    if isinstance(idx, pd.DatetimeIndex):\n",
        "        return pd.Series(idx, index=df.index)\n",
        "    return pd.to_datetime(pd.Series(idx, index=df.index), errors=\"coerce\")\n",
        "\n",
        "def _split_day_night(df, ts_col=\"MM:DD:YYYY hh:mm:ss\"):\n",
        "    ts = _get_timestamp_series(df, ts_col=ts_col)\n",
        "    valid = ts.notna()\n",
        "    hrs = ts.dt.hour\n",
        "    day_mask = valid & (hrs >= 7) & (hrs < 19)\n",
        "    night_mask = valid & ~day_mask\n",
        "    return df.loc[day_mask], df.loc[night_mask]\n",
        "\n",
        "def compute_withinbout_lose_shift(c_df, max_gap_s=120):\n",
        "    try:\n",
        "        if \"Event\" not in c_df.columns or len(c_df) < 2:\n",
        "            return np.nan\n",
        "        events = c_df[\"Event\"].to_numpy()\n",
        "        times = _get_timestamp_series(c_df).to_numpy()\n",
        "        total = shifted = 0\n",
        "        for i in range(len(events) - 1):\n",
        "            curr_evt, next_evt = events[i], events[i + 1]\n",
        "            if curr_evt not in (\"Left\", \"Right\"):\n",
        "                continue\n",
        "            dt_s = (times[i + 1] - times[i]) / np.timedelta64(1, \"s\")\n",
        "            if np.isnan(dt_s) or dt_s > max_gap_s:\n",
        "                continue\n",
        "            if next_evt == \"Pellet\":\n",
        "                continue\n",
        "            if next_evt in (\"Left\", \"Right\"):\n",
        "                total += 1\n",
        "                if next_evt != curr_evt:\n",
        "                    shifted += 1\n",
        "        return (shifted / total) if total > 0 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def compute_withinbout_win_stay(c_df, max_gap_s=120):\n",
        "    try:\n",
        "        if \"Event\" not in c_df.columns or len(c_df) < 3:\n",
        "            return np.nan\n",
        "        events = c_df[\"Event\"].to_numpy()\n",
        "        times = _get_timestamp_series(c_df).to_numpy()\n",
        "        pellet_idx = [i for i in range(1, len(events) - 1) if events[i] == \"Pellet\"]\n",
        "        total = same = 0\n",
        "        for i in pellet_idx:\n",
        "            prev_event, next_event = events[i - 1], events[i + 1]\n",
        "            dt_s = (times[i + 1] - times[i]) / np.timedelta64(1, \"s\")\n",
        "            if not np.isnan(dt_s) and dt_s <= max_gap_s:\n",
        "                if prev_event in (\"Left\", \"Right\") and next_event in (\"Left\", \"Right\"):\n",
        "                    total += 1\n",
        "                    if next_event == prev_event:\n",
        "                        same += 1\n",
        "        return (same / total) if total > 0 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def compute_peak_accuracy(c_df):\n",
        "    try:\n",
        "        if \"Event\" not in c_df.columns or len(c_df) == 0:\n",
        "            return np.nan\n",
        "\n",
        "        events = c_df[\"Event\"]\n",
        "        left_count = (events == \"Left\").sum()\n",
        "        right_count = (events == \"Right\").sum()\n",
        "        total = left_count + right_count\n",
        "\n",
        "        return (left_count / total) * 100 if total > 0 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def estimate_daily_pellets(c_df):\n",
        "    ts = _get_timestamp_series(c_df)\n",
        "    valid_ts = ts.dropna()\n",
        "    if valid_ts.size < 2:\n",
        "        return np.nan\n",
        "    duration_hours = (valid_ts.max() - valid_ts.min()).total_seconds() / 3600.0\n",
        "    if duration_hours <= 0:\n",
        "        return np.nan\n",
        "\n",
        "    pellet_events = np.nan\n",
        "    if \"Pellet_Count\" in c_df.columns and c_df[\"Pellet_Count\"].notna().any():\n",
        "        pc = pd.to_numeric(c_df[\"Pellet_Count\"], errors=\"coerce\")\n",
        "        if pc.notna().any():\n",
        "            diffs = pc.diff().fillna(0).clip(lower=0)\n",
        "            pellet_events = float(diffs.sum())\n",
        "            if pellet_events == 0 and pc.iloc[-1] >= pc.iloc[0]:\n",
        "                pellet_events = float(pc.iloc[-1] - pc.iloc[0])\n",
        "    if (pd.isna(pellet_events)) and (\"Event\" in c_df.columns):\n",
        "        pellet_events = float((c_df[\"Event\"] == \"Pellet\").sum())\n",
        "\n",
        "    if pd.isna(pellet_events):\n",
        "        return np.nan\n",
        "    return (pellet_events / duration_hours) * 24.0\n",
        "\n",
        "# ---------- Prepare metadata (merge once by filename) ----------\n",
        "md = metadata_df.copy()\n",
        "md['filename'] = md['filename'].astype(str).map(_basename)\n",
        "if 'Mouse_ID' in md.columns:\n",
        "    md['Mouse_ID'] = md['Mouse_ID'].astype(str).str.strip()\n",
        "else:\n",
        "    md['Mouse_ID'] = np.nan\n",
        "\n",
        "# Keep only metadata columns we care about; rename to avoid accidental dupes\n",
        "# (add/remove columns as needed)\n",
        "meta_keep = [c for c in md.columns if c in {\"filename\", \"Mouse_ID\", \"Session_type\", \"Cohort\", \"Strain\", \"Sex\"}]\n",
        "md_clean = md[meta_keep].drop_duplicates(subset=[\"filename\"], keep=\"first\")\n",
        "\n",
        "# ---------- Compute metrics on the chosen sessions ----------\n",
        "rows = []\n",
        "for idx in tqdm(range(len(_sessions))):\n",
        "    c_df = _sessions[idx]\n",
        "    file_name = _basename(getattr(c_df, \"name\", f\"File_{idx}\"))\n",
        "\n",
        "    try:\n",
        "        clean_retrieval_time = pd.to_numeric(c_df.get(\"Retrieval_Time\", pd.Series(dtype=float)), errors=\"coerce\")\n",
        "        clean_retrieval_time = clean_retrieval_time[clean_retrieval_time < 5]\n",
        "\n",
        "        clean_poke_time = pd.to_numeric(c_df.get(\"Poke_Time\", pd.Series(dtype=float)), errors=\"coerce\")\n",
        "        clean_poke_time = clean_poke_time[clean_poke_time > 0]\n",
        "\n",
        "        day_df, night_df = _split_day_night(c_df, ts_col=\"MM:DD:YYYY hh:mm:ss\")\n",
        "\n",
        "        row = {\n",
        "            \"filename\": file_name,\n",
        "            \"PeakAccuracy\": compute_peak_accuracy(c_df),\n",
        "            \"Total_pellets\": f3b.count_pellets(c_df),\n",
        "            \"Total_pokes\": f3b.count_pokes(c_df),\n",
        "            \"PokesPerPellet\": f3b.pokes_per_pellet(c_df),\n",
        "            \"RetrievalTime\": clean_retrieval_time.median() if not clean_retrieval_time.empty else np.nan,\n",
        "            \"PokeTime\": clean_poke_time.median() if not clean_poke_time.empty else np.nan,\n",
        "            \"Win-stay\": compute_withinbout_win_stay(c_df),\n",
        "            \"Lose-shift\": compute_withinbout_lose_shift(c_df),\n",
        "            \"daily pellets\": estimate_daily_pellets(c_df),\n",
        "            \"PeakAccuracy_Day\": compute_peak_accuracy(day_df),\n",
        "            \"PeakAccuracy_Night\": compute_peak_accuracy(night_df),\n",
        "            \"Win-stay_Day\": compute_withinbout_win_stay(day_df),\n",
        "            \"Win-stay_Night\": compute_withinbout_win_stay(night_df),\n",
        "            \"Lose-shift_Day\": compute_withinbout_lose_shift(day_df),\n",
        "            \"Lose-shift_Night\": compute_withinbout_lose_shift(night_df),\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed on {file_name} (idx {idx}): {e}\")\n",
        "\n",
        "\n",
        "FR1_metrics = pd.DataFrame(rows)\n",
        "FR1_metrics = FR1_metrics.merge(md_clean, on=\"filename\", how=\"left\")\n",
        "FR1_metrics = FR1_metrics.loc[:, ~FR1_metrics.columns.duplicated()]\n",
        "\n",
        "csv_name = \"FR1_metrics.csv\"\n",
        "FR1_metrics.to_csv(csv_name, index=False)\n",
        "\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "def download_csv(b):\n",
        "    files.download(csv_name)\n",
        "\n",
        "download_button = widgets.Button(\n",
        "    description=\"⬇️ Download summary stats (CSV)\",\n",
        "    button_style=\"primary\",\n",
        ")\n",
        "\n",
        "download_button.on_click(download_csv)\n",
        "display(download_button)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IofVG8Nl644m"
      },
      "id": "IofVG8Nl644m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Female vs Male\n",
        "import os, time, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "except Exception:\n",
        "    colab_files = None\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "GROUP_COL = \"Sex\"\n",
        "metrics = [\n",
        "    \"daily pellets\", \"Total_pokes\", \"PokesPerPellet\", \"PokeTime\",\n",
        "    \"PeakAccuracy\", \"Win-stay\", \"Lose-shift\", \"RetrievalTime\",\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "COLOR_MAP = {\"F\": \"red\", \"M\": \"dodgerblue\"}\n",
        "\n",
        "# -----------------------\n",
        "# Preconditions\n",
        "# -----------------------\n",
        "if \"FR1_metrics\" not in globals() or FR1_metrics is None or FR1_metrics.empty:\n",
        "    raise RuntimeError(\"FR1_metrics is missing/empty. Run the metrics cell first.\")\n",
        "\n",
        "bm = FR1_metrics.copy()\n",
        "\n",
        "# -----------------------\n",
        "# Helper: normalize Sex to F/M/UNK\n",
        "# -----------------------\n",
        "def _norm_sex(x):\n",
        "    s = str(x).strip().upper()\n",
        "    if s in {\"F\", \"FEMALE\", \"FEM\"}: return \"F\"\n",
        "    if s in {\"M\", \"MALE\"}: return \"M\"\n",
        "    return \"UNK\"\n",
        "\n",
        "# -----------------------\n",
        "# Ensure we have Sex (merge from metadata_df if needed)\n",
        "# -----------------------\n",
        "if GROUP_COL not in bm.columns or bm[GROUP_COL].isna().all():\n",
        "\n",
        "    if \"metadata_df\" not in globals() or metadata_df is None or metadata_df.empty:\n",
        "        raise RuntimeError(\"Sex not found in Bandit100_metrics and metadata_df is missing/empty.\")\n",
        "\n",
        "    meta = metadata_df.copy()\n",
        "\n",
        "    # Normalize column names for robust lookup\n",
        "    def _find_col(df, name):\n",
        "        lc = {str(c).strip().lower(): c for c in df.columns}\n",
        "        return lc.get(name.lower(), None)\n",
        "\n",
        "    meta_sex = _find_col(meta, \"Sex\")\n",
        "    if meta_sex is None:\n",
        "        raise RuntimeError(\"metadata_df does not contain a 'Sex' column (case-insensitive).\")\n",
        "\n",
        "    # Prefer merging by Mouse_ID if present in both\n",
        "    meta_mouse = _find_col(meta, \"Mouse_ID\")\n",
        "    bm_mouse   = _find_col(bm, \"Mouse_ID\")\n",
        "\n",
        "    merged = None\n",
        "\n",
        "    if meta_mouse is not None and bm_mouse is not None:\n",
        "        meta_key = meta[[meta_mouse, meta_sex]].copy()\n",
        "        meta_key.columns = [\"Mouse_ID\", \"Sex\"]\n",
        "        meta_key[\"Mouse_ID\"] = meta_key[\"Mouse_ID\"].astype(str).str.strip()\n",
        "        meta_key = meta_key.dropna(subset=[\"Mouse_ID\"]).drop_duplicates(\"Mouse_ID\")\n",
        "\n",
        "        bm[\"Mouse_ID\"] = bm[bm_mouse].astype(str).str.strip()\n",
        "        merged = bm.merge(meta_key, on=\"Mouse_ID\", how=\"left\")\n",
        "\n",
        "    # Fallback: merge by filename basename if Mouse_ID isn’t available\n",
        "    if merged is None or merged[\"Sex\"].isna().all():\n",
        "        # build/ensure filename columns\n",
        "        if \"filename\" not in bm.columns:\n",
        "            if \"File\" in bm.columns:\n",
        "                bm[\"filename\"] = bm[\"File\"].astype(str)\n",
        "            else:\n",
        "                raise RuntimeError(\"Need either Mouse_ID or filename/File in Bandit100_metrics to merge Sex.\")\n",
        "\n",
        "        if \"filename\" not in meta.columns:\n",
        "            # if metadata_df already has filename, great; otherwise cannot fallback\n",
        "            raise RuntimeError(\"Cannot fallback merge: metadata_df has no filename column.\")\n",
        "\n",
        "        bm[\"file_base\"]   = bm[\"filename\"].astype(str).apply(lambda p: os.path.basename(p))\n",
        "        meta[\"file_base\"] = meta[\"filename\"].astype(str).apply(lambda p: os.path.basename(p))\n",
        "\n",
        "        meta_key = meta[[\"file_base\", meta_sex]].copy()\n",
        "        meta_key.columns = [\"file_base\", \"Sex\"]\n",
        "        meta_key = meta_key.dropna(subset=[\"file_base\"]).drop_duplicates(\"file_base\")\n",
        "\n",
        "        merged = bm.merge(meta_key, on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "\n",
        "    bm = merged\n",
        "\n",
        "# Normalize Sex values\n",
        "bm[GROUP_COL] = bm[GROUP_COL].apply(_norm_sex)\n",
        "\n",
        "# Keep only F/M for plotting (drop UNK)\n",
        "bm = bm[bm[GROUP_COL].isin([\"F\", \"M\"])].copy()\n",
        "if bm.empty:\n",
        "    raise RuntimeError(\"After merging/normalizing Sex, no rows with Sex in {F, M} were found.\")\n",
        "\n",
        "# -----------------------\n",
        "# Long format\n",
        "# -----------------------\n",
        "value_vars = [m for m in metrics if m in bm.columns]\n",
        "if not value_vars:\n",
        "    raise RuntimeError(\"None of the expected metric columns were found in Bandit100_metrics.\")\n",
        "\n",
        "id_vars = [c for c in [\"filename\", \"Mouse_ID\", \"Strain\", GROUP_COL] if c in bm.columns]\n",
        "long_df = bm.melt(id_vars=id_vars, value_vars=value_vars, var_name=\"metric\", value_name=\"value\")\n",
        "\n",
        "# consistent order\n",
        "groups = [g for g in [\"F\", \"M\"] if g in long_df[GROUP_COL].unique()]\n",
        "if len(groups) < 2:\n",
        "    raise RuntimeError(f\"Need both F and M present to compare; found: {groups}\")\n",
        "\n",
        "# -----------------------\n",
        "# Stats\n",
        "# -----------------------\n",
        "def welch_p(a, b):\n",
        "    a = pd.Series(a, dtype=float).dropna()\n",
        "    b = pd.Series(b, dtype=float).dropna()\n",
        "    if len(a) < 2 or len(b) < 2:\n",
        "        return np.nan\n",
        "    return float(pg.ttest(a, b, paired=False)[\"p-val\"].iat[0])\n",
        "\n",
        "# -----------------------\n",
        "# Plot UI\n",
        "# -----------------------\n",
        "out = widgets.Output()\n",
        "save_btn = widgets.Button(description=\"Save PDF\", button_style=\"success\")\n",
        "_last_fig = None\n",
        "\n",
        "def run_plots():\n",
        "    global _last_fig\n",
        "    with out:\n",
        "        clear_output()\n",
        "\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(8, 6), constrained_layout=True)\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for i, metric in enumerate(metrics):\n",
        "            ax = axes[i]\n",
        "            if metric not in value_vars:\n",
        "                ax.set_axis_off()\n",
        "                continue\n",
        "\n",
        "            dfm = long_df[long_df[\"metric\"] == metric].dropna(subset=[\"value\"])\n",
        "\n",
        "            pal = {g: COLOR_MAP[g] for g in groups}\n",
        "\n",
        "            sns.barplot(\n",
        "                data=dfm, x=GROUP_COL, y=\"value\",\n",
        "                order=groups, ci=None, alpha=0.6,\n",
        "                palette=pal, ax=ax\n",
        "            )\n",
        "            sns.stripplot(\n",
        "                data=dfm, x=GROUP_COL, y=\"value\",\n",
        "                order=groups,\n",
        "                color=\"white\", edgecolor=\"black\",\n",
        "                linewidth=1, size=6,\n",
        "                alpha=0.35, jitter=True, ax=ax\n",
        "            )\n",
        "\n",
        "            a = dfm.loc[dfm[GROUP_COL] == groups[0], \"value\"]\n",
        "            b = dfm.loc[dfm[GROUP_COL] == groups[1], \"value\"]\n",
        "            p = welch_p(a, b)\n",
        "            label = (\n",
        "                \"p<0.001\" if np.isfinite(p) and p < 0.001\n",
        "                else (f\"p={p:.3f}\" if np.isfinite(p) else \"p=NA\")\n",
        "            )\n",
        "\n",
        "            ax.set_title(\"\")\n",
        "            ax.set_xlabel(\"\")\n",
        "            ax.set_ylabel(metric, fontsize=12)\n",
        "            ax.text(0.5, 1.02, label, transform=ax.transAxes, ha=\"center\", va=\"bottom\")\n",
        "\n",
        "            sns.despine(ax=ax)\n",
        "\n",
        "        plt.show()\n",
        "        _last_fig = fig\n",
        "\n",
        "def save_plots(_=None):\n",
        "    if _last_fig is None:\n",
        "        with out:\n",
        "            print(\"Nothing to save yet.\")\n",
        "        return\n",
        "    fname = f\"metrics_grid_{int(time.time())}.pdf\"\n",
        "    _last_fig.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
        "    with out:\n",
        "        print(f\"Saved {fname}\")\n",
        "    if colab_files is not None:\n",
        "        colab_files.download(fname)\n",
        "\n",
        "save_btn.on_click(save_plots)\n",
        "\n",
        "display(save_btn)\n",
        "display(out)\n",
        "\n",
        "run_plots()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OmUjcQq8SMtY"
      },
      "id": "OmUjcQq8SMtY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO ADD AVERAGE LEARNING CURVES\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HqaHI6O9cwzm"
      },
      "id": "HqaHI6O9cwzm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Update \"Bandit80\" in below called to be \"FR1\""
      ],
      "metadata": {
        "id": "z0EMaT3YHBqE"
      },
      "id": "z0EMaT3YHBqE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO ADD HEATMAP OF BANDIT/FR1\n",
        "\n",
        "#@title Correlation plots\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "Bandit100metrics  = pd.read_csv(r\"https://raw.githubusercontent.com/KravitzLab/Murrell2025/refs/heads/main/Data/SummaryStats/Bandit100_metrics.csv\")\n",
        "Bandit80metrics = pd.read_csv(r\"https://raw.githubusercontent.com/KravitzLab/Murrell2025/refs/heads/main/Data/SummaryStats/FR1_metrics.csv\")\n",
        "\n",
        "print(\"Loaded Bandit80metrics:\", Bandit80metrics.shape)\n",
        "print(\"Loaded Bandit100_metrics:\", Bandit100metrics.shape)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2) Match metrics to Key_Df by Mouse_ID\n",
        "#    (assumes Key_Df already exists in the environment)\n",
        "# -------------------------------------------------------------------\n",
        "if \"Key_Df\" not in globals():\n",
        "    raise RuntimeError(\"Key_Df is not defined. Build/rematch Key_Df first.\")\n",
        "\n",
        "# Ensure Mouse_ID is string and stripped in all tables\n",
        "def _clean_mouse_id(df, col=\"Mouse_ID\"):\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "    return df\n",
        "\n",
        "Key_Df = _clean_mouse_id(Key_Df.copy())\n",
        "Bandit80metrics  = _clean_mouse_id(Bandit80metrics.copy())\n",
        "Bandit100_metrics = _clean_mouse_id(Bandit100metrics.copy())\n",
        "\n",
        "# Add suffixes to metric columns so we can have both in the same table\n",
        "id_cols = {\"Mouse_ID\", \"filename\", \"Session_type\", \"Cohort\", \"Strain\", \"Sex\"}\n",
        "\n",
        "def _add_suffix_to_metrics(df, suffix):\n",
        "    df = df.copy()\n",
        "    rename_map = {\n",
        "        c: f\"{c}{suffix}\"\n",
        "        for c in df.columns\n",
        "        if c not in id_cols\n",
        "    }\n",
        "    return df.rename(columns=rename_map)\n",
        "\n",
        "b80_suffixed  = _add_suffix_to_metrics(Bandit80metrics,  \"_Bandit80\")\n",
        "b100_suffixed = _add_suffix_to_metrics(Bandit100_metrics, \"_Bandit100\")\n",
        "\n",
        "# Merge the two metrics tables on Mouse_ID\n",
        "metrics_merged = pd.merge(\n",
        "    b80_suffixed,\n",
        "    b100_suffixed,\n",
        "    on=\"Mouse_ID\",\n",
        "    how=\"outer\",\n",
        "    suffixes=(\"\", \"_dup\")  # should not be needed because we already suffixed\n",
        ")\n",
        "\n",
        "print(\"Merged metrics table shape:\", metrics_merged.shape)\n",
        "\n",
        "# Merge metrics into Key_Df\n",
        "Key_with_metrics = pd.merge(\n",
        "    Key_Df,\n",
        "    metrics_merged,\n",
        "    on=\"Mouse_ID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(\"Key_with_metrics shape:\", Key_with_metrics.shape)\n",
        "\n",
        "# For convenience, use this DataFrame for correlations/plots:\n",
        "df = Key_with_metrics\n",
        "df = Key_with_metrics.copy()\n",
        "\n",
        "# In case Sex wasn't carried through for some reason, re-merge it from Key_Df\n",
        "if \"Sex\" not in df.columns and \"Mouse_ID\" in df.columns:\n",
        "    sex_map = Key_Df[[\"Mouse_ID\", \"Sex\"]].copy()\n",
        "    sex_map[\"Mouse_ID\"] = sex_map[\"Mouse_ID\"].astype(str).str.strip()\n",
        "    df[\"Mouse_ID\"] = df[\"Mouse_ID\"].astype(str).str.strip()\n",
        "    df = df.merge(sex_map, on=\"Mouse_ID\", how=\"left\")\n",
        "\n",
        "print(\"Columns in df:\", df.columns.tolist())\n",
        "# -------------------------------------------------------------------\n",
        "# 3) Correlate Bandit80 vs Bandit100 metrics and plot\n",
        "#    Columns expected to exist after suffixing:\n",
        "#    PeakAccuracy_Bandit80, PeakAccuracy_Bandit100, etc.\n",
        "# -------------------------------------------------------------------\n",
        "pairs = [\n",
        "    (\"PeakAccuracy_Bandit80\", \"PeakAccuracy_Bandit100\", \"PeakAccuracy\"),\n",
        "    (\"Win-stay_Bandit80\",     \"Win-stay_Bandit100\",     \"Win-stay\"),\n",
        "    (\"Lose-shift_Bandit80\",   \"Lose-shift_Bandit100\",   \"Lose-shift\"),\n",
        "]\n",
        "\n",
        "# Per-subplot axis limits (edit these to taste)\n",
        "axis_limits = {\n",
        "    \"Peak Accuracy\": {\"x\": (0.4, 1.0), \"y\": (0.4, 1.0)},\n",
        "    \"Win-stay\":      {\"x\": (0.4, 1.0), \"y\": (0.4, 1.0)},\n",
        "    \"Lose-shift\":    {\"x\": (0.0, 1.0), \"y\": (0.0, 1.0)},\n",
        "}\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=False, sharey=False)\n",
        "\n",
        "for ax, (col80, col100, label) in zip(axes, pairs):\n",
        "    needed_cols = [col80, col100, \"Sex\"]\n",
        "    missing = [c for c in needed_cols if c not in df.columns]\n",
        "    if missing:\n",
        "        print(f\"Skipping {label}: missing columns {missing}\")\n",
        "        continue\n",
        "\n",
        "    sub = df[needed_cols].dropna()\n",
        "    if len(sub) < 2:\n",
        "        print(f\"{col80} vs {col100}: not enough data (n={len(sub)})\")\n",
        "        continue\n",
        "\n",
        "    # Normalize Sex values to 'Male' / 'Female'\n",
        "    sub = sub.copy()\n",
        "    sub[\"Sex_plot\"] = (\n",
        "        sub[\"Sex\"]\n",
        "        .astype(str)\n",
        "        .str.strip()\n",
        "        .str.lower()\n",
        "        .map({\"m\": \"Male\", \"f\": \"Female\", \"male\": \"Male\", \"female\": \"Female\"})\n",
        "    )\n",
        "    sub = sub.dropna(subset=[\"Sex_plot\"])\n",
        "    if len(sub) < 2:\n",
        "        print(f\"{col80} vs {col100}: not enough data after Sex mapping (n={len(sub)})\")\n",
        "        continue\n",
        "\n",
        "    # Pearson correlation\n",
        "    r, p = pearsonr(sub[col80], sub[col100])\n",
        "\n",
        "    # Regression line in grey on this axis\n",
        "    sns.regplot(\n",
        "        data=sub,\n",
        "        x=col80,\n",
        "        y=col100,\n",
        "        ci=None,\n",
        "        scatter=False,\n",
        "        line_kws={\"color\": \"grey\", \"linewidth\": 2},\n",
        "        ax=ax,\n",
        "    )\n",
        "\n",
        "    # Sex-colored scatter on this axis\n",
        "    sns.scatterplot(\n",
        "        data=sub,\n",
        "        x=col80,\n",
        "        y=col100,\n",
        "        hue=\"Sex_plot\",\n",
        "        palette={\"Male\": \"red\", \"Female\": \"dodgerblue\"},\n",
        "        edgecolor=\"black\",\n",
        "        ax=ax,\n",
        "        legend=False if ax is not axes[-1] else True,  # only show legend on last\n",
        "    )\n",
        "\n",
        "    # Set per-subplot axis limits\n",
        "    lims = axis_limits.get(label, None)\n",
        "    if lims is not None:\n",
        "        ax.set_xlim(*lims[\"x\"])\n",
        "        ax.set_ylim(*lims[\"y\"])\n",
        "\n",
        "    ax.set_xlabel(f\"{label} (Bandit80)\")\n",
        "    ax.set_ylabel(f\"{label} (Bandit100)\")\n",
        "    ax.set_title(f\"{label}\\n n={len(sub)}, r={r:.3f}, p={p:.3g}\")\n",
        "    sns.despine()\n",
        "    ax.grid (False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uqW-cGDEDQg2"
      },
      "id": "uqW-cGDEDQg2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Correlation heatmap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. Ensure merged dataset exists\n",
        "# ------------------------------------------------------------------\n",
        "if \"Key_with_metrics\" not in globals():\n",
        "    raise RuntimeError(\"Key_with_metrics not found — run merge first.\")\n",
        "\n",
        "df = Key_with_metrics.copy()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1b. Drop any columns containing \"Day\" or \"Night\", and \"Total_pellets\"\n",
        "# ------------------------------------------------------------------\n",
        "cols_to_drop = [c for c in df.columns if (\"Day\" in c) or (\"Night\" in c)]\n",
        "cols_to_drop.extend([\"Total_pellets_Bandit100\", \"Total_pellets_Bandit80\"])\n",
        "\n",
        "df = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2. Identify all Bandit80 and Bandit100 metric columns\n",
        "# ------------------------------------------------------------------\n",
        "b80_cols = sorted([c for c in df.columns if c.endswith(\"_Bandit80\")])\n",
        "b100_cols = sorted([c for c in df.columns if c.endswith(\"_Bandit100\")])\n",
        "\n",
        "# Keep only numeric columns\n",
        "b80_cols = [c for c in b80_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
        "b100_cols = [c for c in b100_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3. Build a cross-correlation matrix: rows=Bandit100, columns=Bandit80\n",
        "# ------------------------------------------------------------------\n",
        "cross_corr = pd.DataFrame(index=b100_cols, columns=b80_cols, dtype=float)\n",
        "\n",
        "for col100 in b100_cols:\n",
        "    for col80 in b80_cols:\n",
        "        sub = df[[col100, col80]].dropna()\n",
        "        if len(sub) < 2:\n",
        "            cross_corr.loc[col100, col80] = np.nan\n",
        "        else:\n",
        "            cross_corr.loc[col100, col80] = sub[col100].corr(sub[col80])\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3b. Order rows/columns by number of strong correlations (|r| >= 0.30)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "threshold = 0.30\n",
        "\n",
        "# Boolean mask of strong correlations\n",
        "strong_mask = cross_corr.abs() >= threshold\n",
        "\n",
        "# Convert row/column names to base metric names\n",
        "row_base = cross_corr.index.str.replace(\"_Bandit100\", \"\", regex=False)\n",
        "col_base = cross_corr.columns.str.replace(\"_Bandit80\", \"\", regex=False)\n",
        "\n",
        "# Count strong correlations per base metric\n",
        "row_counts = (\n",
        "    strong_mask\n",
        "    .groupby(row_base)\n",
        "    .sum()\n",
        "    .sum(axis=1)\n",
        ")\n",
        "\n",
        "col_counts = (\n",
        "    strong_mask\n",
        "    .groupby(col_base, axis=1)\n",
        "    .sum()\n",
        "    .sum(axis=0)\n",
        ")\n",
        "\n",
        "# Combine counts (rows + columns) → single ordering\n",
        "total_counts = row_counts.add(col_counts, fill_value=0)\n",
        "\n",
        "base_order = total_counts.sort_values(ascending=False).index.tolist()\n",
        "\n",
        "# Rebuild ordered row/column labels\n",
        "row_order = [f\"{b}_Bandit100\" for b in base_order if f\"{b}_Bandit100\" in cross_corr.index]\n",
        "col_order = [f\"{b}_Bandit80\"  for b in base_order if f\"{b}_Bandit80\"  in cross_corr.columns]\n",
        "\n",
        "# Reorder the matrix\n",
        "cross_corr_sorted = cross_corr.loc[row_order, col_order]\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "# Base layer: all correlations in light grey, NO annotations\n",
        "ax = sns.heatmap(\n",
        "    cross_corr_sorted,\n",
        "    cmap=\"Greys_r\",\n",
        "    vmin=-1, vmax=1,\n",
        "    center=0,\n",
        "    annot=False,\n",
        "    linewidths=0.5,\n",
        "    cbar=False,\n",
        ")\n",
        "\n",
        "# Second layer: only strong correlations (|r| >= threshold), colored + annotated\n",
        "strong_corr = cross_corr_sorted.where(cross_corr_sorted.abs() >= threshold)\n",
        "\n",
        "sns.heatmap(\n",
        "    strong_corr,\n",
        "    cmap=\"coolwarm\",\n",
        "    vmin=-1, vmax=1,\n",
        "    center=0,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    linewidths=0.5,\n",
        "    mask=strong_corr.isna(),\n",
        "    cbar_kws={\"shrink\": 0.6},\n",
        "    ax=ax,\n",
        ")\n",
        "# Remove \"_Bandit80\" and \"_Bandit100\" from axis labels\n",
        "clean_x = [label.get_text().replace(\"_Bandit80\", \"\").replace(\"_Bandit100\", \"\")\n",
        "           for label in ax.get_xticklabels()]\n",
        "clean_y = [label.get_text().replace(\"_Bandit80\", \"\").replace(\"_Bandit100\", \"\")\n",
        "           for label in ax.get_yticklabels()]\n",
        "\n",
        "ax.set_xticklabels(clean_x, rotation=45, ha=\"right\")\n",
        "ax.set_yticklabels(clean_y)\n",
        "\n",
        "plt.title(\"\")\n",
        "plt.xlabel(\"Bandit80 Metrics\")\n",
        "plt.ylabel(\"Bandit100 Metrics\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0MzTzEPKEm7h"
      },
      "id": "0MzTzEPKEm7h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_kzGX-UFkv7"
      },
      "id": "i_kzGX-UFkv7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (Spyder)",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}