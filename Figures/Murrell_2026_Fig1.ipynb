{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/Murrell2025/blob/main/Murrell_2026_Fig1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Murrell 2026 Figure 1\n",
        "<br>\n",
        "<img src=\"https://fed3bandit.readthedocs.io/en/latest/_static/fed3bandit_logo1.svg\" width=\"200\" />\n",
        "\n",
        "Authors: Chantelle Murrell<br>\n",
        "Updated: 12-30-25  "
      ],
      "metadata": {
        "id": "ru9_Mj2OpjhT"
      },
      "id": "ru9_Mj2OpjhT"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install libraries and import them {\"run\":\"auto\"}\n",
        "\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Packages to ensure are installed (add others here if you like)\n",
        "packages = {\n",
        "    \"fed3\": \"git+https://github.com/earnestt1234/fed3.git\",\n",
        "    \"fed3bandit\": \"fed3bandit\",\n",
        "    \"pingouin\": \"pingouin\",\n",
        "    \"ipydatagrid\": \"ipydatagrid\",\n",
        "    \"openpyxl\": \"openpyxl\",\n",
        "}\n",
        "\n",
        "for name, source in packages.items():\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        print(f\"Installing {name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", source])\n",
        "\n",
        "# ----------------------------\n",
        "# Imports\n",
        "# ----------------------------\n",
        "# Standard library\n",
        "import copy\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import threading\n",
        "import time\n",
        "import warnings\n",
        "import zipfile\n",
        "import requests\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "from os.path import basename, splitext\n",
        "\n",
        "# Third-party\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "import fed3\n",
        "import fed3.plot as fplot\n",
        "import fed3bandit as f3b\n",
        "from scipy.stats import f_oneway\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "from google.colab import files\n",
        "try:\n",
        "    from tqdm.auto import tqdm   # nice in notebooks; falls back to std tqdm on console\n",
        "except Exception:\n",
        "    # safe no-op fallback if tqdm isn't installed\n",
        "    def tqdm(x):\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams.update({'font.size': 12, 'figure.autolayout': True})\n",
        "plt.rcParams['figure.figsize'] = [6, 4]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "print(\"Packages installed and imports ready.\")\n"
      ],
      "metadata": {
        "id": "c_vNg7dvVfCh",
        "cellView": "form",
        "collapsed": true
      },
      "id": "c_vNg7dvVfCh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import FED3 Bandit 100-0 Data\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "import os, zipfile, shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from google.colab import output as colab_output\n",
        "    colab_output.enable_custom_widget_manager()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "zip_url = \"https://github.com/KravitzLab/Murrell2025/raw/refs/heads/main/Data/Bandit100.zip\"\n",
        "key_url = \"https://github.com/KravitzLab/Murrell2025/raw/refs/heads/main/Data/Murrell2026_Key.csv\"\n",
        "\n",
        "zip_dir = \"/content/Murrell2025_zipdata\"\n",
        "zip_path = os.path.join(zip_dir, \"Bandit100.zip\")\n",
        "extract_root = os.path.join(zip_dir, \"Bandit100_extracted\")\n",
        "key_path = os.path.join(zip_dir, \"Murrell2026_Key.csv\")\n",
        "\n",
        "os.makedirs(zip_dir, exist_ok=True)\n",
        "\n",
        "# download + unzip (fresh each run)\n",
        "if os.path.exists(zip_path):\n",
        "    os.remove(zip_path)\n",
        "if os.path.isdir(extract_root):\n",
        "    shutil.rmtree(extract_root)\n",
        "\n",
        "print(\"Importing github.com/KravitzLab/Murrell2025/Data/Bandit100.zip ...\")\n",
        "urlretrieve(zip_url, zip_path)\n",
        "\n",
        "os.makedirs(extract_root, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(extract_root)\n",
        "\n",
        "# if the zip contains a Bandit100 folder, use it; otherwise use extract_root\n",
        "bandit_root = os.path.join(extract_root, \"Bandit100\")\n",
        "local_parent_path = bandit_root if os.path.isdir(bandit_root) else extract_root\n",
        "\n",
        "# load CSVs\n",
        "feds, loaded_files, session_types = [], [], []\n",
        "\n",
        "for dirpath, _, filenames in os.walk(local_parent_path):\n",
        "    for file_name in sorted(filenames):\n",
        "        if file_name.lower().endswith(\".csv\"):\n",
        "            file_path = os.path.join(dirpath, file_name)\n",
        "            strain_name = os.path.basename(os.path.dirname(file_path))\n",
        "\n",
        "            df = fed3.load(file_path)\n",
        "            df.name = file_name\n",
        "            df[\"Strain\"] = strain_name\n",
        "            df[\"SourceFile\"] = file_name\n",
        "\n",
        "            feds.append(df)\n",
        "            loaded_files.append(file_path)\n",
        "\n",
        "            st = df[\"Session_Type\"].dropna().astype(str)\n",
        "            session_types.append(st.iloc[0] if len(st) else None)\n",
        "\n",
        "print(f\"Loaded {len(feds)} CSV files.\")\n",
        "\n",
        "# download + load key\n",
        "urlretrieve(key_url, key_path)\n",
        "\n",
        "key_df = pd.read_csv(key_path, encoding=\"utf-8-sig\")\n",
        "key_df[\"Mouse_ID\"] = key_df[\"Mouse_ID\"].astype(str).str.strip()\n",
        "\n",
        "# match Mouse_ID by substring in filename base\n",
        "def _base_lower(p):\n",
        "    return os.path.splitext(os.path.basename(p))[0].lower()\n",
        "\n",
        "files_df = pd.DataFrame({\"filename\": loaded_files, \"Session_type\": session_types})\n",
        "files_df[\"_base\"] = files_df[\"filename\"].map(_base_lower)\n",
        "\n",
        "mouse_ids = (\n",
        "    key_df[\"Mouse_ID\"]\n",
        "    .dropna().astype(str).str.strip()\n",
        "    .replace(\"\", np.nan).dropna().unique().tolist()\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for fname, base in zip(files_df[\"filename\"], files_df[\"_base\"]):\n",
        "    hits = [mid for mid in mouse_ids if mid.lower() in base]\n",
        "    rows.append({\"filename\": fname, \"Mouse_ID\": hits[0] if len(hits) else None})\n",
        "\n",
        "matched = pd.DataFrame(rows)\n",
        "\n",
        "Key_Df = (\n",
        "    files_df.drop(columns=[\"_base\"])\n",
        "    .merge(matched, on=\"filename\", how=\"left\")\n",
        "    .merge(key_df.drop_duplicates(\"Mouse_ID\"), on=\"Mouse_ID\", how=\"left\")\n",
        ")\n",
        "\n",
        "# display\n",
        "grid = DataGrid(\n",
        "    Key_Df.reset_index(drop=True),\n",
        "    editable=True,\n",
        "    selection_mode=\"cell\",\n",
        "    layout={\"height\": \"420px\"},\n",
        "    base_row_size=28,\n",
        "    base_column_size=120,\n",
        ")\n",
        "grid.default_renderer = TextRenderer(text_wrap=True)\n",
        "display(grid)"
      ],
      "metadata": {
        "id": "_3hqwt-oejwc",
        "cellView": "form"
      },
      "id": "_3hqwt-oejwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot individual files (males are blue, females are red)\n",
        "# ----- Inputs -----\n",
        "assert 'feds' in globals() and isinstance(feds, list) and len(feds) > 0, \"No FED3 files loaded.\"\n",
        "assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "\n",
        "TIMESTAMP_COL_CANON = \"MM:DD:YYYY hh:mm:ss\"  # primary target column name\n",
        "\n",
        "# ----- Helpers -----\n",
        "def _find_time_col(df):\n",
        "    # exact match first\n",
        "    if TIMESTAMP_COL_CANON in df.columns:\n",
        "        return TIMESTAMP_COL_CANON\n",
        "    # tolerant search (case/space-insensitive)\n",
        "    lc = {str(c).strip().lower(): c for c in df.columns}\n",
        "    for key in lc:\n",
        "        if key.replace(\" \", \"\") in {\"mm:dd:yyyyhh:mm:ss\", \"mm:dd:yyyy_hh:mm:ss\", \"mm/dd/yyyyhh:mm:ss\"}:\n",
        "            return lc[key]\n",
        "    return None\n",
        "\n",
        "def _parse_ts(series):\n",
        "    # robust parsing; coerce errors to NaT\n",
        "    return pd.to_datetime(series, errors=\"coerce\", infer_datetime_format=True)\n",
        "\n",
        "\n",
        "# ----- Plotting\n",
        "files_list = feds\n",
        "\n",
        "# metadata_df = copy of Key_Df\n",
        "metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "if 'filename' in metadata_df.columns:\n",
        "    metadata_df['filename'] = metadata_df['filename'].astype(str).map(os.path.basename)\n",
        "\n",
        "def _coerce_numeric_col(df, col, clip_upper=None, na_map=None):\n",
        "    if col not in df.columns:\n",
        "        return\n",
        "    s = df[col]\n",
        "    if na_map:\n",
        "        s = s.replace(na_map)\n",
        "    s = pd.to_numeric(s, errors='coerce')\n",
        "    if clip_upper is not None:\n",
        "        s.loc[s > clip_upper] = np.nan\n",
        "    df[col] = s\n",
        "\n",
        "def _plot_file_core(file_index):\n",
        "    df = files_list[file_index].copy()\n",
        "    full_name = getattr(df, 'name', f\"File_{file_index}\")\n",
        "    file_basename = os.path.basename(str(full_name))\n",
        "\n",
        "    # Preserve original index once\n",
        "    if \"Original_Timestamp\" not in df.columns:\n",
        "        df[\"Original_Timestamp\"] = df.index\n",
        "\n",
        "    # Attach metadata by filename (matching already done upstream)\n",
        "    meta_row = None\n",
        "    if 'filename' in metadata_df.columns:\n",
        "        mr = metadata_df.loc[metadata_df['filename'] == file_basename]\n",
        "        if not mr.empty:\n",
        "            meta_row = mr.iloc[0]\n",
        "    if meta_row is not None:\n",
        "        for col in meta_row.index:\n",
        "            if col == 'filename':\n",
        "                continue\n",
        "            if col not in df.columns:\n",
        "                df[col] = meta_row[col]\n",
        "            else:\n",
        "                if pd.isna(df[col]).all() and pd.notna(meta_row[col]):\n",
        "                    df[col] = meta_row[col]\n",
        "\n",
        "    # Time + cleanup\n",
        "    try:\n",
        "        df['timestamp'] = pd.to_datetime(df.index)\n",
        "    except Exception:\n",
        "        df['timestamp'] = np.arange(len(df))\n",
        "\n",
        "    _coerce_numeric_col(df, 'Poke_Time', clip_upper=2)\n",
        "    _coerce_numeric_col(df, 'Retrieval_Time', na_map={\"Timed_out\": np.nan})\n",
        "\n",
        "    if len(df) == 0:\n",
        "        print(f\"[!] Empty cropped dataframe for {file_basename}. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    # Behavioral traces (needs f3b)\n",
        "    true_left = f3b.true_probs(df, offset=5)[0]\n",
        "    mouse_left = f3b.binned_paction(df, window=10)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 3))\n",
        "    ax.plot(np.arange(len(true_left)), true_left, color=\"black\", linewidth=2, alpha=0.5)\n",
        "\n",
        "    color = \"dodgerblue\"\n",
        "    if 'Sex' in df.columns and pd.notna(df['Sex']).any():\n",
        "        try:\n",
        "            color = \"red\" if str(df['Sex'].iloc[0]).strip().lower().startswith(\"f\") else \"dodgerblue\"\n",
        "        except Exception:\n",
        "            pass\n",
        "    ax.plot(np.arange(len(mouse_left)), mouse_left, color=color, linewidth=3, alpha=0.7)\n",
        "\n",
        "    # ---- Clean look: remove ticks, labels, spines ----\n",
        "    ax.set_xlabel(\"\")                 # no x label\n",
        "    ax.set_ylabel(\"\")                 # no y label\n",
        "    ax.tick_params(axis=\"both\", which=\"both\",\n",
        "                   bottom=False, top=False, left=False, right=False,\n",
        "                   labelbottom=False, labelleft=False)\n",
        "    for s in ax.spines.values():      # remove axis lines\n",
        "        s.set_visible(False)\n",
        "\n",
        "    # ---- Textual y \"labels\" at y=1 and y=0 (not ticks) ----\n",
        "    ax.text(-0.01, 1.0, \"Right\", transform=ax.get_yaxis_transform(),\n",
        "            ha=\"right\", va=\"center\")\n",
        "    ax.text(-0.01, 0.0, \"Left\",  transform=ax.get_yaxis_transform(),\n",
        "            ha=\"right\", va=\"center\")\n",
        "\n",
        "    # ---- Title-area arrow above the axes ----\n",
        "    # spans full width; adjust y (1.08–1.15) if you need more/less space\n",
        "    ax.annotate(\"\",\n",
        "                xy=(0.95, 1.05), xytext=(0.05, 1.05),\n",
        "                xycoords=\"axes fraction\",\n",
        "                arrowprops=dict(arrowstyle=\"->\", lw=5, color=\"0.6\"))\n",
        "    # Optional: small caption above the arrow (example: duration text)\n",
        "    ax.text(0.4, 1.15, \"3 days\", transform=ax.transAxes, ha=\"left\", va=\"bottom\", color=\"0.5\")\n",
        "    sns.despine(left=True, bottom=True, top=True, right=True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ----- Simple UI: slider + status + output -----\n",
        "N = len(files_list)\n",
        "assert N > 0, \"No files available after cropping.\"\n",
        "idx_slider = widgets.IntSlider(min=0, max=max(0, N-1), step=1, value=0, description='File', continuous_update=True)\n",
        "status_lbl = widgets.HTML()\n",
        "out = widgets.Output()\n",
        "\n",
        "def _status(idx):\n",
        "    name = getattr(files_list[idx], 'name', f\"File_{idx}\")\n",
        "    return f\"Index: <b>{idx}</b> &nbsp;|&nbsp; File: <code>{os.path.basename(str(name))}</code> &nbsp;|&nbsp; Rows: {len(files_list[idx])}\"\n",
        "\n",
        "def _render(*_):\n",
        "    idx = int(idx_slider.value)\n",
        "    status_lbl.value = _status(idx)\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        _plot_file_core(idx)\n",
        "\n",
        "idx_slider.observe(_render, names='value')\n",
        "display(widgets.VBox([idx_slider, status_lbl, out]))\n",
        "_render()\n"
      ],
      "metadata": {
        "id": "qeZfA9tXWngI",
        "cellView": "form"
      },
      "id": "qeZfA9tXWngI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Analyze Bandit metrics\n",
        "from pathlib import Path\n",
        "import os\n",
        "def _find_time_col(df):\n",
        "    for c in [\"MM:DD:YYYY hh:mm:ss\", \"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _get_timestamp_series(df, ts_col=\"MM:DD:YYYY hh:mm:ss\"):\n",
        "    import pandas as pd\n",
        "    if ts_col in df.columns:\n",
        "        ts = pd.to_datetime(df[ts_col], format=\"%m:%d:%Y %H:%M:%S\", errors=\"coerce\")\n",
        "        return pd.Series(ts, index=df.index)\n",
        "    for cand in [\"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "        if cand in df.columns:\n",
        "            ts = pd.to_datetime(df[cand], errors=\"coerce\")\n",
        "            return pd.Series(ts, index=df.index)\n",
        "    idx = df.index\n",
        "    if isinstance(idx, pd.DatetimeIndex):\n",
        "        return pd.Series(idx, index=df.index)\n",
        "    return pd.to_datetime(pd.Series(idx, index=df.index), errors=\"coerce\")\n",
        "\n",
        "def _crop_last_24h(df):\n",
        "    import pandas as pd\n",
        "    dfc = df.copy()\n",
        "    ts_col = _find_time_col(dfc)\n",
        "\n",
        "    if ts_col is not None:\n",
        "        ts_series = pd.to_datetime(dfc[ts_col], errors=\"coerce\", infer_datetime_format=True)\n",
        "        ts_series = pd.Series(ts_series, index=dfc.index)\n",
        "    else:\n",
        "        try:\n",
        "            ts_idx = pd.to_datetime(dfc.index, errors=\"coerce\", infer_datetime_format=True)\n",
        "            ts_series = pd.Series(ts_idx, index=dfc.index)\n",
        "        except Exception:\n",
        "            return df  # no usable timestamps → return original\n",
        "\n",
        "    if ts_series.isna().all():\n",
        "        return df\n",
        "\n",
        "    end = ts_series.max()\n",
        "    if pd.isna(end):\n",
        "        return df\n",
        "    start = end - pd.Timedelta(hours=24)\n",
        "\n",
        "    mask = ts_series.between(start, end, inclusive=\"both\")\n",
        "    cropped = dfc.loc[mask]\n",
        "\n",
        "    if hasattr(df, \"name\"):\n",
        "        cropped.name = df.name\n",
        "    return cropped\n",
        "\n",
        "def build_feds_cropped(sessions):\n",
        "    \"\"\"Return a list of sessions cropped to their last 24h.\"\"\"\n",
        "    return [_crop_last_24h(d) for d in sessions]\n",
        "\n",
        "# Use it like this:\n",
        "assert 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0, \"No FED3 files available.\"\n",
        "feds_cropped = build_feds_cropped(feds)\n",
        "\n",
        "# Prefer cropped sessions downstream\n",
        "_sessions = list(feds_cropped) if len(feds_cropped) > 0 else list(feds)\n",
        "# ----- Build feds_cropped -----\n",
        "feds_cropped = [_crop_last_24h(d) for d in feds]\n",
        "# ---------- Inputs ----------\n",
        "# Prefer cropped sessions\n",
        "if 'feds_cropped' in globals() and isinstance(feds_cropped, (list, tuple)) and len(feds_cropped) > 0:\n",
        "    _sessions = list(feds_cropped)\n",
        "else:\n",
        "    assert 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0, \"No FED3 files available.\"\n",
        "    _sessions = list(feds)\n",
        "\n",
        "# metadata_df from Key_Df\n",
        "if 'metadata_df' not in globals() or not isinstance(metadata_df, pd.DataFrame):\n",
        "    assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "    metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "\n",
        "def _basename(pathlike) -> str:\n",
        "    s = str(pathlike).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "def _get_timestamp_series(df, ts_col=\"MM:DD:YYYY hh:mm:ss\"):\n",
        "    if ts_col in df.columns:\n",
        "        ts = pd.to_datetime(df[ts_col], format=\"%m:%d:%Y %H:%M:%S\", errors=\"coerce\")\n",
        "        return pd.Series(ts, index=df.index)\n",
        "    for cand in [\"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "        if cand in df.columns:\n",
        "            ts = pd.to_datetime(df[cand], errors=\"coerce\")\n",
        "            return pd.Series(ts, index=df.index)\n",
        "    idx = df.index\n",
        "    if isinstance(idx, pd.DatetimeIndex):\n",
        "        return pd.Series(idx, index=df.index)\n",
        "    return pd.to_datetime(pd.Series(idx, index=df.index), errors=\"coerce\")\n",
        "\n",
        "def _split_day_night(df, ts_col=\"MM:DD:YYYY hh:mm:ss\"):\n",
        "    ts = _get_timestamp_series(df, ts_col=ts_col)\n",
        "    valid = ts.notna()\n",
        "    hrs = ts.dt.hour\n",
        "    day_mask = valid & (hrs >= 7) & (hrs < 19)\n",
        "    night_mask = valid & ~day_mask\n",
        "    return df.loc[day_mask], df.loc[night_mask]\n",
        "\n",
        "def compute_withinbout_lose_shift(c_df, max_gap_s=120):\n",
        "    try:\n",
        "        if \"Event\" not in c_df.columns or len(c_df) < 2:\n",
        "            return np.nan\n",
        "        events = c_df[\"Event\"].to_numpy()\n",
        "        times = _get_timestamp_series(c_df).to_numpy()\n",
        "        total = shifted = 0\n",
        "        for i in range(len(events) - 1):\n",
        "            curr_evt, next_evt = events[i], events[i + 1]\n",
        "            if curr_evt not in (\"Left\", \"Right\"):\n",
        "                continue\n",
        "            dt_s = (times[i + 1] - times[i]) / np.timedelta64(1, \"s\")\n",
        "            if np.isnan(dt_s) or dt_s > max_gap_s:\n",
        "                continue\n",
        "            if next_evt == \"Pellet\":\n",
        "                continue\n",
        "            if next_evt in (\"Left\", \"Right\"):\n",
        "                total += 1\n",
        "                if next_evt != curr_evt:\n",
        "                    shifted += 1\n",
        "        return (shifted / total) if total > 0 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def compute_withinbout_win_stay(c_df, max_gap_s=120):\n",
        "    try:\n",
        "        if \"Event\" not in c_df.columns or len(c_df) < 3:\n",
        "            return np.nan\n",
        "        events = c_df[\"Event\"].to_numpy()\n",
        "        times = _get_timestamp_series(c_df).to_numpy()\n",
        "        pellet_idx = [i for i in range(1, len(events) - 1) if events[i] == \"Pellet\"]\n",
        "        total = same = 0\n",
        "        for i in pellet_idx:\n",
        "            prev_event, next_event = events[i - 1], events[i + 1]\n",
        "            dt_s = (times[i + 1] - times[i]) / np.timedelta64(1, \"s\")\n",
        "            if not np.isnan(dt_s) and dt_s <= max_gap_s:\n",
        "                if prev_event in (\"Left\", \"Right\") and next_event in (\"Left\", \"Right\"):\n",
        "                    total += 1\n",
        "                    if next_event == prev_event:\n",
        "                        same += 1\n",
        "        return (same / total) if total > 0 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def compute_peak_accuracy(c_df):\n",
        "    try:\n",
        "        rev_avg = f3b.reversal_peh(c_df, (-10, 10), True)\n",
        "        if len(rev_avg) == 0:\n",
        "            return np.nan\n",
        "        return float(np.mean(rev_avg[:10])) if len(rev_avg) >= 10 else float(np.mean(rev_avg))\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def estimate_daily_pellets(c_df):\n",
        "    ts = _get_timestamp_series(c_df)\n",
        "    valid_ts = ts.dropna()\n",
        "    if valid_ts.size < 2:\n",
        "        return np.nan\n",
        "    duration_hours = (valid_ts.max() - valid_ts.min()).total_seconds() / 3600.0\n",
        "    if duration_hours <= 0:\n",
        "        return np.nan\n",
        "\n",
        "    pellet_events = np.nan\n",
        "    if \"Pellet_Count\" in c_df.columns and c_df[\"Pellet_Count\"].notna().any():\n",
        "        pc = pd.to_numeric(c_df[\"Pellet_Count\"], errors=\"coerce\")\n",
        "        if pc.notna().any():\n",
        "            diffs = pc.diff().fillna(0).clip(lower=0)\n",
        "            pellet_events = float(diffs.sum())\n",
        "            if pellet_events == 0 and pc.iloc[-1] >= pc.iloc[0]:\n",
        "                pellet_events = float(pc.iloc[-1] - pc.iloc[0])\n",
        "    if (pd.isna(pellet_events)) and (\"Event\" in c_df.columns):\n",
        "        pellet_events = float((c_df[\"Event\"] == \"Pellet\").sum())\n",
        "\n",
        "    if pd.isna(pellet_events):\n",
        "        return np.nan\n",
        "    return (pellet_events / duration_hours) * 24.0\n",
        "\n",
        "# ---------- Prepare metadata (merge once by filename) ----------\n",
        "md = metadata_df.copy()\n",
        "md['filename'] = md['filename'].astype(str).map(_basename)\n",
        "if 'Mouse_ID' in md.columns:\n",
        "    md['Mouse_ID'] = md['Mouse_ID'].astype(str).str.strip()\n",
        "else:\n",
        "    md['Mouse_ID'] = np.nan\n",
        "\n",
        "# Keep only metadata columns we care about; rename to avoid accidental dupes\n",
        "# (add/remove columns as needed)\n",
        "meta_keep = [c for c in md.columns if c in {\"filename\", \"Mouse_ID\", \"Session_type\", \"Cohort\", \"Strain\", \"Sex\"}]\n",
        "md_clean = md[meta_keep].drop_duplicates(subset=[\"filename\"], keep=\"first\")\n",
        "\n",
        "# ---------- Compute metrics on the chosen sessions ----------\n",
        "rows = []\n",
        "for idx in tqdm(range(len(_sessions))):\n",
        "    c_df = _sessions[idx]\n",
        "    file_name = _basename(getattr(c_df, \"name\", f\"File_{idx}\"))\n",
        "\n",
        "    try:\n",
        "        clean_retrieval_time = pd.to_numeric(c_df.get(\"Retrieval_Time\", pd.Series(dtype=float)), errors=\"coerce\")\n",
        "        clean_retrieval_time = clean_retrieval_time[clean_retrieval_time < 5]\n",
        "\n",
        "        clean_poke_time = pd.to_numeric(c_df.get(\"Poke_Time\", pd.Series(dtype=float)), errors=\"coerce\")\n",
        "        clean_poke_time = clean_poke_time[clean_poke_time > 0]\n",
        "\n",
        "        day_df, night_df = _split_day_night(c_df, ts_col=\"MM:DD:YYYY hh:mm:ss\")\n",
        "\n",
        "        row = {\n",
        "            \"filename\": file_name,\n",
        "            \"PeakAccuracy\": compute_peak_accuracy(c_df),\n",
        "            \"Total_pellets\": f3b.count_pellets(c_df),\n",
        "            \"Total_pokes\": f3b.count_pokes(c_df),\n",
        "            \"PokesPerPellet\": f3b.pokes_per_pellet(c_df),\n",
        "            \"RetrievalTime\": clean_retrieval_time.median() if not clean_retrieval_time.empty else np.nan,\n",
        "            \"PokeTime\": clean_poke_time.median() if not clean_poke_time.empty else np.nan,\n",
        "            \"Win-stay\": compute_withinbout_win_stay(c_df),\n",
        "            \"Lose-shift\": compute_withinbout_lose_shift(c_df),\n",
        "            \"daily pellets\": estimate_daily_pellets(c_df),\n",
        "            \"PeakAccuracy_Day\": compute_peak_accuracy(day_df),\n",
        "            \"PeakAccuracy_Night\": compute_peak_accuracy(night_df),\n",
        "            \"Win-stay_Day\": compute_withinbout_win_stay(day_df),\n",
        "            \"Win-stay_Night\": compute_withinbout_win_stay(night_df),\n",
        "            \"Lose-shift_Day\": compute_withinbout_lose_shift(day_df),\n",
        "            \"Lose-shift_Night\": compute_withinbout_lose_shift(night_df),\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed on {file_name} (idx {idx}): {e}\")\n",
        "\n",
        "\n",
        "Bandit100_metrics = pd.DataFrame(rows)\n",
        "Bandit100_metrics = Bandit100_metrics.merge(md_clean, on=\"filename\", how=\"left\")\n",
        "Bandit100_metrics = Bandit100_metrics.loc[:, ~Bandit100_metrics.columns.duplicated()]\n",
        "\n",
        "csv_name = \"Bandit100_metrics.csv\"\n",
        "Bandit100_metrics.to_csv(csv_name, index=False)\n",
        "\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "def download_csv(b):\n",
        "    files.download(csv_name)\n",
        "\n",
        "download_button = widgets.Button(\n",
        "    description=\"⬇️ Download summary stats (CSV)\",\n",
        "    button_style=\"primary\",\n",
        ")\n",
        "\n",
        "download_button.on_click(download_csv)\n",
        "display(download_button)\n"
      ],
      "metadata": {
        "id": "gTvKLp-fXgZk",
        "collapsed": true,
        "cellView": "form"
      },
      "id": "gTvKLp-fXgZk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Female vs Male (with FDR correction across plots)\n",
        "import os, time, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "except Exception:\n",
        "    colab_files = None\n",
        "\n",
        "from statsmodels.stats.multitest import multipletests  # <-- FDR\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "GROUP_COL = \"Sex\"\n",
        "metrics = [\n",
        "    \"daily pellets\", \"Total_pokes\",\n",
        "    \"PeakAccuracy\", \"Win-stay\",\n",
        "]\n",
        "\n",
        "COLOR_MAP = {\"F\": \"red\", \"M\": \"dodgerblue\"}\n",
        "\n",
        "# -----------------------\n",
        "# Preconditions\n",
        "# -----------------------\n",
        "if \"Bandit100_metrics\" not in globals() or Bandit100_metrics is None or Bandit100_metrics.empty:\n",
        "    raise RuntimeError(\"Bandit100_metrics is missing/empty. Run the metrics cell first.\")\n",
        "\n",
        "bm = Bandit100_metrics.copy()\n",
        "\n",
        "# -----------------------\n",
        "# Helper: normalize Sex to F/M/UNK\n",
        "# -----------------------\n",
        "def _norm_sex(x):\n",
        "    s = str(x).strip().upper()\n",
        "    if s in {\"F\", \"FEMALE\", \"FEM\"}: return \"F\"\n",
        "    if s in {\"M\", \"MALE\"}: return \"M\"\n",
        "    return \"UNK\"\n",
        "\n",
        "# -----------------------\n",
        "# Ensure we have Sex (merge from metadata_df if needed)\n",
        "# -----------------------\n",
        "if GROUP_COL not in bm.columns or bm[GROUP_COL].isna().all():\n",
        "\n",
        "    if \"metadata_df\" not in globals() or metadata_df is None or metadata_df.empty:\n",
        "        raise RuntimeError(\"Sex not found in Bandit100_metrics and metadata_df is missing/empty.\")\n",
        "\n",
        "    meta = metadata_df.copy()\n",
        "\n",
        "    # Normalize column names for robust lookup\n",
        "    def _find_col(df, name):\n",
        "        lc = {str(c).strip().lower(): c for c in df.columns}\n",
        "        return lc.get(name.lower(), None)\n",
        "\n",
        "    meta_sex = _find_col(meta, \"Sex\")\n",
        "    if meta_sex is None:\n",
        "        raise RuntimeError(\"metadata_df does not contain a 'Sex' column (case-insensitive).\")\n",
        "\n",
        "    # Prefer merging by Mouse_ID if present in both\n",
        "    meta_mouse = _find_col(meta, \"Mouse_ID\")\n",
        "    bm_mouse   = _find_col(bm, \"Mouse_ID\")\n",
        "\n",
        "    merged = None\n",
        "\n",
        "    if meta_mouse is not None and bm_mouse is not None:\n",
        "        meta_key = meta[[meta_mouse, meta_sex]].copy()\n",
        "        meta_key.columns = [\"Mouse_ID\", \"Sex\"]\n",
        "        meta_key[\"Mouse_ID\"] = meta_key[\"Mouse_ID\"].astype(str).str.strip()\n",
        "        meta_key = meta_key.dropna(subset=[\"Mouse_ID\"]).drop_duplicates(\"Mouse_ID\")\n",
        "\n",
        "        bm[\"Mouse_ID\"] = bm[bm_mouse].astype(str).str.strip()\n",
        "        merged = bm.merge(meta_key, on=\"Mouse_ID\", how=\"left\")\n",
        "\n",
        "    # Fallback: merge by filename basename if Mouse_ID isn’t available\n",
        "    if merged is None or merged[\"Sex\"].isna().all():\n",
        "        # build/ensure filename columns\n",
        "        if \"filename\" not in bm.columns:\n",
        "            if \"File\" in bm.columns:\n",
        "                bm[\"filename\"] = bm[\"File\"].astype(str)\n",
        "            else:\n",
        "                raise RuntimeError(\"Need either Mouse_ID or filename/File in Bandit100_metrics to merge Sex.\")\n",
        "\n",
        "        if \"filename\" not in meta.columns:\n",
        "            raise RuntimeError(\"Cannot fallback merge: metadata_df has no filename column.\")\n",
        "\n",
        "        bm[\"file_base\"]   = bm[\"filename\"].astype(str).apply(lambda p: os.path.basename(p))\n",
        "        meta[\"file_base\"] = meta[\"filename\"].astype(str).apply(lambda p: os.path.basename(p))\n",
        "\n",
        "        meta_key = meta[[\"file_base\", meta_sex]].copy()\n",
        "        meta_key.columns = [\"file_base\", \"Sex\"]\n",
        "        meta_key = meta_key.dropna(subset=[\"file_base\"]).drop_duplicates(\"file_base\")\n",
        "\n",
        "        merged = bm.merge(meta_key, on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "\n",
        "    bm = merged\n",
        "\n",
        "# Normalize Sex values\n",
        "bm[GROUP_COL] = bm[GROUP_COL].apply(_norm_sex)\n",
        "\n",
        "# Keep only F/M for plotting (drop UNK)\n",
        "bm = bm[bm[GROUP_COL].isin([\"F\", \"M\"])].copy()\n",
        "if bm.empty:\n",
        "    raise RuntimeError(\"After merging/normalizing Sex, no rows with Sex in {F, M} were found.\")\n",
        "\n",
        "# -----------------------\n",
        "# Long format\n",
        "# -----------------------\n",
        "value_vars = [m for m in metrics if m in bm.columns]\n",
        "if not value_vars:\n",
        "    raise RuntimeError(\"None of the expected metric columns were found in Bandit100_metrics.\")\n",
        "\n",
        "id_vars = [c for c in [\"filename\", \"Mouse_ID\", \"Strain\", GROUP_COL] if c in bm.columns]\n",
        "long_df = bm.melt(id_vars=id_vars, value_vars=value_vars, var_name=\"metric\", value_name=\"value\")\n",
        "\n",
        "# consistent order\n",
        "groups = [g for g in [\"F\", \"M\"] if g in long_df[GROUP_COL].unique()]\n",
        "if len(groups) < 2:\n",
        "    raise RuntimeError(f\"Need both F and M present to compare; found: {groups}\")\n",
        "\n",
        "# -----------------------\n",
        "# Stats\n",
        "# -----------------------\n",
        "def welch_p(a, b):\n",
        "    a = pd.Series(a, dtype=float).dropna()\n",
        "    b = pd.Series(b, dtype=float).dropna()\n",
        "    if len(a) < 2 or len(b) < 2:\n",
        "        return np.nan\n",
        "    return float(pg.ttest(a, b, paired=False)[\"p-val\"].iat[0])\n",
        "\n",
        "# -----------------------\n",
        "# Precompute p-values + FDR correction across plotted metrics\n",
        "# -----------------------\n",
        "raw_pvals = {}\n",
        "for metric in metrics:\n",
        "    if metric not in value_vars:\n",
        "        raw_pvals[metric] = np.nan\n",
        "        continue\n",
        "    dfm = long_df[long_df[\"metric\"] == metric].dropna(subset=[\"value\"])\n",
        "    a = dfm.loc[dfm[GROUP_COL] == groups[0], \"value\"]\n",
        "    b = dfm.loc[dfm[GROUP_COL] == groups[1], \"value\"]\n",
        "    raw_pvals[metric] = welch_p(a, b)\n",
        "\n",
        "valid_metrics = [m for m, p in raw_pvals.items() if np.isfinite(p)]\n",
        "valid_pvals = [raw_pvals[m] for m in valid_metrics]\n",
        "\n",
        "pvals_fdr_map = {}\n",
        "if len(valid_pvals) > 0:\n",
        "    _, pvals_fdr, _, _ = multipletests(valid_pvals, alpha=0.05, method=\"fdr_bh\")\n",
        "    pvals_fdr_map = dict(zip(valid_metrics, pvals_fdr))\n",
        "\n",
        "# -----------------------\n",
        "# Plot UI\n",
        "# -----------------------\n",
        "out = widgets.Output()\n",
        "save_btn = widgets.Button(description=\"Save PDF\", button_style=\"success\")\n",
        "_last_fig = None\n",
        "\n",
        "def run_plots():\n",
        "    global _last_fig\n",
        "    with out:\n",
        "        clear_output()\n",
        "\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(8, 6), constrained_layout=True)\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for i, metric in enumerate(metrics):\n",
        "            ax = axes[i]\n",
        "            if metric not in value_vars:\n",
        "                ax.set_axis_off()\n",
        "                continue\n",
        "\n",
        "            dfm = long_df[long_df[\"metric\"] == metric].dropna(subset=[\"value\"])\n",
        "\n",
        "            pal = {g: COLOR_MAP[g] for g in groups}\n",
        "\n",
        "            sns.barplot(\n",
        "                data=dfm, x=GROUP_COL, y=\"value\",\n",
        "                order=groups, ci=None, alpha=0.6,\n",
        "                palette=pal, ax=ax\n",
        "            )\n",
        "            sns.stripplot(\n",
        "                data=dfm, x=GROUP_COL, y=\"value\",\n",
        "                order=groups,\n",
        "                color=\"white\", edgecolor=\"black\",\n",
        "                linewidth=1, size=6,\n",
        "                alpha=0.35, jitter=True, ax=ax\n",
        "            )\n",
        "\n",
        "            p_fdr = pvals_fdr_map.get(metric, np.nan)\n",
        "            label = (\n",
        "                \"FDR p<0.001\" if np.isfinite(p_fdr) and p_fdr < 0.001\n",
        "                else (f\"FDR p={p_fdr:.3f}\" if np.isfinite(p_fdr) else \"FDR p=NA\")\n",
        "            )\n",
        "\n",
        "            ax.set_title(\"\")\n",
        "            ax.set_xlabel(\"\")\n",
        "            ax.set_ylabel(metric, fontsize=12)\n",
        "            ax.text(0.5, 1.02, label, transform=ax.transAxes, ha=\"center\", va=\"bottom\")\n",
        "\n",
        "            sns.despine(ax=ax)\n",
        "\n",
        "        plt.show()\n",
        "        _last_fig = fig\n",
        "\n",
        "def save_plots(_=None):\n",
        "    if _last_fig is None:\n",
        "        with out:\n",
        "            print(\"Nothing to save yet.\")\n",
        "        return\n",
        "    fname = f\"metrics_grid_{int(time.time())}.pdf\"\n",
        "    _last_fig.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
        "    with out:\n",
        "        print(f\"Saved {fname}\")\n",
        "    if colab_files is not None:\n",
        "        colab_files.download(fname)\n",
        "\n",
        "save_btn.on_click(save_plots)\n",
        "\n",
        "display(save_btn)\n",
        "display(out)\n",
        "\n",
        "run_plots()\n"
      ],
      "metadata": {
        "id": "OmUjcQq8SMtY",
        "cellView": "form"
      },
      "id": "OmUjcQq8SMtY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Accuracy around switches\n",
        "\n",
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Config ---\n",
        "TRIALS = 11\n",
        "COLOR_MAP = {\"F\": \"red\", \"M\": \"dodgerblue\"}\n",
        "\n",
        "# --- Preconditions ---\n",
        "if 'feds' not in globals() or not isinstance(feds, (list, tuple)) or len(feds) == 0:\n",
        "    raise RuntimeError(\"No FED3 sessions found in `feds`.\")\n",
        "if 'metadata_df' not in globals() or metadata_df is None or metadata_df.empty:\n",
        "    raise RuntimeError(\"metadata_df is missing/empty. Build it from the Key first.\")\n",
        "\n",
        "# --- Helpers ---\n",
        "def _basename(x): return os.path.basename(str(x))\n",
        "\n",
        "def _norm_sex(x):\n",
        "    s = str(x).strip().upper()\n",
        "    if s in {\"F\", \"FEMALE\", \"FEM\"}: return \"F\"\n",
        "    if s in {\"M\", \"MALE\"}: return \"M\"\n",
        "    return \"UNK\"\n",
        "\n",
        "def _find_col(df, name):\n",
        "    lc = {str(c).strip().lower(): c for c in df.columns}\n",
        "    return lc.get(name.lower(), None)\n",
        "\n",
        "def _extract_mouse_id_from_sess_name(name):\n",
        "    \"\"\"\n",
        "    Expected patterns like:\n",
        "      CDKL5_005_180_69_Bandit80_20251013.csv  -> CDKL5_005_180_69\n",
        "      ChowHFD_670_Bandit80_20250728.csv       -> ChowHFD_670\n",
        "    Returns the ID part (no extension).\n",
        "    \"\"\"\n",
        "    base = _basename(name)\n",
        "    base = re.sub(r\"\\.csv$\", \"\", base, flags=re.IGNORECASE)\n",
        "\n",
        "    # Strip trailing task/date: _Bandit80_YYYYMMDD, _FR1_YYYYMMDD, etc.\n",
        "    base = re.sub(r\"_(Bandit80|Bandit100|FR1|PR1)_\\d{8}$\", \"\", base, flags=re.IGNORECASE)\n",
        "    return base\n",
        "\n",
        "def _is_empty(x):\n",
        "    if x is None: return True\n",
        "    try:\n",
        "        return len(x) == 0\n",
        "    except Exception:\n",
        "        try:\n",
        "            return np.size(x) == 0\n",
        "        except Exception:\n",
        "            return True\n",
        "\n",
        "# --- Build Sex lookup from metadata_df ---\n",
        "meta = metadata_df.copy()\n",
        "meta_sex = _find_col(meta, \"Sex\")\n",
        "if meta_sex is None:\n",
        "    raise RuntimeError(\"metadata_df does not contain a 'Sex' column (case-insensitive).\")\n",
        "meta_mouse = _find_col(meta, \"Mouse_ID\")\n",
        "\n",
        "sex_lookup = {}\n",
        "\n",
        "# Prefer Mouse_ID mapping if present\n",
        "if meta_mouse is not None:\n",
        "    tmp = meta[[meta_mouse, meta_sex]].copy()\n",
        "    tmp.columns = [\"Mouse_ID\", \"Sex\"]\n",
        "    tmp[\"Mouse_ID\"] = tmp[\"Mouse_ID\"].astype(str).str.strip()\n",
        "    tmp[\"Sex\"] = tmp[\"Sex\"].apply(_norm_sex)\n",
        "    tmp = tmp.dropna(subset=[\"Mouse_ID\"]).drop_duplicates(\"Mouse_ID\")\n",
        "    sex_lookup = dict(zip(tmp[\"Mouse_ID\"], tmp[\"Sex\"]))\n",
        "\n",
        "# Fallback: if metadata_df has filename, build filename->sex map too\n",
        "file_sex_lookup = {}\n",
        "if \"filename\" in meta.columns:\n",
        "    tmp2 = meta[[\"filename\", meta_sex]].copy()\n",
        "    tmp2[\"file_base\"] = tmp2[\"filename\"].astype(str).apply(_basename)\n",
        "    tmp2[\"Sex\"] = tmp2[meta_sex].apply(_norm_sex)\n",
        "    tmp2 = tmp2.dropna(subset=[\"file_base\"]).drop_duplicates(\"file_base\")\n",
        "    file_sex_lookup = dict(zip(tmp2[\"file_base\"], tmp2[\"Sex\"]))\n",
        "\n",
        "# --- Build rev_df ---\n",
        "rows = []\n",
        "for i, sess in enumerate(feds):\n",
        "    sess_name = getattr(sess, \"name\", f\"session_{i}\")\n",
        "    base = _basename(sess_name)\n",
        "\n",
        "    # Get an ID from session filename and look up Sex\n",
        "    mouse_id = _extract_mouse_id_from_sess_name(base)\n",
        "    sex = sex_lookup.get(mouse_id, \"UNK\")\n",
        "\n",
        "    # fallback: try direct filename basename lookup (only if metadata has filename)\n",
        "    if sex == \"UNK\" and file_sex_lookup:\n",
        "        sex = file_sex_lookup.get(base, \"UNK\")\n",
        "\n",
        "    # compute peri-switch trials using your helper\n",
        "    try:\n",
        "        peh = f3b.reversal_peh(sess, (-TRIALS, TRIALS), return_avg=False)\n",
        "    except Exception as e:\n",
        "        print(f\"[skip] {base}: reversal_peh failed: {e}\")\n",
        "        continue\n",
        "\n",
        "    if _is_empty(peh) or sex not in {\"F\", \"M\"}:\n",
        "        continue\n",
        "\n",
        "    for tr in list(peh):\n",
        "        arr = np.asarray(tr).ravel()\n",
        "        for t, v in enumerate(arr):\n",
        "            rows.append({\n",
        "                \"Timepoint\": t - TRIALS + 1,\n",
        "                \"Value\": float(v) if np.isfinite(v) else np.nan,\n",
        "                \"Sex\": sex\n",
        "            })\n",
        "\n",
        "rev_df = pd.DataFrame(rows)\n",
        "rev_df = rev_df[np.isfinite(rev_df[\"Value\"])]\n",
        "rev_df = rev_df[rev_df[\"Timepoint\"] != 0]  # optional\n",
        "if rev_df.empty:\n",
        "    raise RuntimeError(\"No peri-switch data produced (after filtering to F/M).\")\n",
        "\n",
        "# --- Plot ---\n",
        "group_order = [g for g in [\"F\", \"M\"] if g in rev_df[\"Sex\"].unique()]\n",
        "palette = {g: COLOR_MAP[g] for g in group_order}\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "ax = sns.lineplot(\n",
        "    data=rev_df.sort_values([\"Sex\", \"Timepoint\"]),\n",
        "    x=\"Timepoint\",\n",
        "    y=\"Value\",\n",
        "    hue=\"Sex\",\n",
        "    hue_order=group_order,\n",
        "    palette=palette,\n",
        "    estimator=\"mean\",\n",
        "    errorbar=\"se\",\n",
        "    n_boot=0,\n",
        "    lw=2\n",
        ")\n",
        "\n",
        "ax.axvline(x=0, color=\"darkgrey\", linestyle=\"--\", linewidth=1.25)\n",
        "ymin, ymax = ax.get_ylim()\n",
        "ax.text(0.5, ymin + 0.95*(ymax - ymin), \"Switch\", color=\"darkgrey\",\n",
        "        fontsize=12, ha=\"left\", va=\"top\")\n",
        "\n",
        "ax.set_xlabel(\"Trials from switch\")\n",
        "ax.set_ylabel(\"Accuracy (%)\")\n",
        "ax.set_title(\"\")\n",
        "ax.legend(title=\"\", frameon=False)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VJSi78otlrKS",
        "cellView": "form"
      },
      "id": "VJSi78otlrKS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Day vs. Night metrics\n",
        "import os, time, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "except Exception:\n",
        "    colab_files = None\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "GROUP_COL = \"Sex\"\n",
        "metrics = [\n",
        "    \"PeakAccuracy_Night\", \"Win-stay_Night\", \"Lose-shift_Night\",\n",
        "    \"PeakAccuracy_Day\", \"Win-stay_Day\", \"Lose-shift_Day\",\n",
        "]\n",
        "COLOR_MAP = {\"F\": \"red\", \"M\": \"dodgerblue\"}\n",
        "\n",
        "# -----------------------\n",
        "# Preconditions\n",
        "# -----------------------\n",
        "if \"Bandit100_metrics\" not in globals() or Bandit100_metrics is None or Bandit100_metrics.empty:\n",
        "    raise RuntimeError(\"Bandit100_metrics is missing/empty. Run the metrics cell first.\")\n",
        "\n",
        "bm = Bandit100_metrics.copy()\n",
        "\n",
        "# -----------------------\n",
        "# Helper: normalize Sex to F/M/UNK\n",
        "# -----------------------\n",
        "def _norm_sex(x):\n",
        "    s = str(x).strip().upper()\n",
        "    if s in {\"F\", \"FEMALE\", \"FEM\"}: return \"F\"\n",
        "    if s in {\"M\", \"MALE\"}: return \"M\"\n",
        "    return \"UNK\"\n",
        "\n",
        "# -----------------------\n",
        "# Ensure we have Sex (merge from metadata_df if needed)\n",
        "# -----------------------\n",
        "if GROUP_COL not in bm.columns or bm[GROUP_COL].isna().all():\n",
        "\n",
        "    if \"metadata_df\" not in globals() or metadata_df is None or metadata_df.empty:\n",
        "        raise RuntimeError(\"Sex not found in Bandit100_metrics and metadata_df is missing/empty.\")\n",
        "\n",
        "    meta = metadata_df.copy()\n",
        "\n",
        "    # Normalize column names for robust lookup\n",
        "    def _find_col(df, name):\n",
        "        lc = {str(c).strip().lower(): c for c in df.columns}\n",
        "        return lc.get(name.lower(), None)\n",
        "\n",
        "    meta_sex = _find_col(meta, \"Sex\")\n",
        "    if meta_sex is None:\n",
        "        raise RuntimeError(\"metadata_df does not contain a 'Sex' column (case-insensitive).\")\n",
        "\n",
        "    # Prefer merging by Mouse_ID if present in both\n",
        "    meta_mouse = _find_col(meta, \"Mouse_ID\")\n",
        "    bm_mouse   = _find_col(bm, \"Mouse_ID\")\n",
        "\n",
        "    merged = None\n",
        "\n",
        "    if meta_mouse is not None and bm_mouse is not None:\n",
        "        meta_key = meta[[meta_mouse, meta_sex]].copy()\n",
        "        meta_key.columns = [\"Mouse_ID\", \"Sex\"]\n",
        "        meta_key[\"Mouse_ID\"] = meta_key[\"Mouse_ID\"].astype(str).str.strip()\n",
        "        meta_key = meta_key.dropna(subset=[\"Mouse_ID\"]).drop_duplicates(\"Mouse_ID\")\n",
        "\n",
        "        bm[\"Mouse_ID\"] = bm[bm_mouse].astype(str).str.strip()\n",
        "        merged = bm.merge(meta_key, on=\"Mouse_ID\", how=\"left\")\n",
        "\n",
        "    # Fallback: merge by filename basename if Mouse_ID isn’t available\n",
        "    if merged is None or merged[\"Sex\"].isna().all():\n",
        "        # build/ensure filename columns\n",
        "        if \"filename\" not in bm.columns:\n",
        "            if \"File\" in bm.columns:\n",
        "                bm[\"filename\"] = bm[\"File\"].astype(str)\n",
        "            else:\n",
        "                raise RuntimeError(\"Need either Mouse_ID or filename/File in Bandit100_metrics to merge Sex.\")\n",
        "\n",
        "        if \"filename\" not in meta.columns:\n",
        "            # if metadata_df already has filename, great; otherwise cannot fallback\n",
        "            raise RuntimeError(\"Cannot fallback merge: metadata_df has no filename column.\")\n",
        "\n",
        "        bm[\"file_base\"]   = bm[\"filename\"].astype(str).apply(lambda p: os.path.basename(p))\n",
        "        meta[\"file_base\"] = meta[\"filename\"].astype(str).apply(lambda p: os.path.basename(p))\n",
        "\n",
        "        meta_key = meta[[\"file_base\", meta_sex]].copy()\n",
        "        meta_key.columns = [\"file_base\", \"Sex\"]\n",
        "        meta_key = meta_key.dropna(subset=[\"file_base\"]).drop_duplicates(\"file_base\")\n",
        "\n",
        "        merged = bm.merge(meta_key, on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "\n",
        "    bm = merged\n",
        "\n",
        "# Normalize Sex values\n",
        "bm[GROUP_COL] = bm[GROUP_COL].apply(_norm_sex)\n",
        "\n",
        "# Keep only F/M for plotting (drop UNK)\n",
        "bm = bm[bm[GROUP_COL].isin([\"F\", \"M\"])].copy()\n",
        "if bm.empty:\n",
        "    raise RuntimeError(\"After merging/normalizing Sex, no rows with Sex in {F, M} were found.\")\n",
        "\n",
        "# -----------------------\n",
        "# Long format\n",
        "# -----------------------\n",
        "value_vars = [m for m in metrics if m in bm.columns]\n",
        "if not value_vars:\n",
        "    raise RuntimeError(\"None of the expected metric columns were found in Bandit100_metrics.\")\n",
        "\n",
        "id_vars = [c for c in [\"filename\", \"Mouse_ID\", \"Strain\", GROUP_COL] if c in bm.columns]\n",
        "long_df = bm.melt(id_vars=id_vars, value_vars=value_vars, var_name=\"metric\", value_name=\"value\")\n",
        "\n",
        "# consistent order\n",
        "groups = [g for g in [\"F\", \"M\"] if g in long_df[GROUP_COL].unique()]\n",
        "if len(groups) < 2:\n",
        "    raise RuntimeError(f\"Need both F and M present to compare; found: {groups}\")\n",
        "\n",
        "# -----------------------\n",
        "# Stats\n",
        "# -----------------------\n",
        "def welch_p(a, b):\n",
        "    a = pd.Series(a, dtype=float).dropna()\n",
        "    b = pd.Series(b, dtype=float).dropna()\n",
        "    if len(a) < 2 or len(b) < 2:\n",
        "        return np.nan\n",
        "    return float(pg.ttest(a, b, paired=False)[\"p-val\"].iat[0])\n",
        "\n",
        "# -----------------------\n",
        "# Plot UI\n",
        "# -----------------------\n",
        "out = widgets.Output()\n",
        "save_btn = widgets.Button(description=\"Save PDF\", button_style=\"success\")\n",
        "_last_fig = None\n",
        "\n",
        "def run_plots():\n",
        "    global _last_fig\n",
        "    with out:\n",
        "        clear_output()\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(6, 6), constrained_layout=True)\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for i, metric in enumerate(metrics):\n",
        "            ax = axes[i]\n",
        "            if metric not in value_vars:\n",
        "                ax.set_axis_off()\n",
        "                continue\n",
        "\n",
        "            dfm = long_df[long_df[\"metric\"] == metric].dropna(subset=[\"value\"])\n",
        "\n",
        "            pal = {g: COLOR_MAP[g] for g in groups}\n",
        "\n",
        "            sns.barplot(\n",
        "                data=dfm, x=GROUP_COL, y=\"value\",\n",
        "                order=groups, ci=None, alpha=0.6,\n",
        "                palette=pal, ax=ax\n",
        "            )\n",
        "            sns.stripplot(\n",
        "                data=dfm, x=GROUP_COL, y=\"value\",\n",
        "                order=groups,\n",
        "                color=\"white\", edgecolor=\"black\",\n",
        "                linewidth=1, size=6,\n",
        "                alpha=0.35, jitter=True, ax=ax\n",
        "            )\n",
        "\n",
        "            a = dfm.loc[dfm[GROUP_COL] == groups[0], \"value\"]\n",
        "            b = dfm.loc[dfm[GROUP_COL] == groups[1], \"value\"]\n",
        "            p = welch_p(a, b)\n",
        "            label = (\n",
        "                \"p<0.001\" if np.isfinite(p) and p < 0.001\n",
        "                else (f\"p={p:.3f}\" if np.isfinite(p) else \"p=NA\")\n",
        "            )\n",
        "\n",
        "            ax.set_title(\"\")\n",
        "            ax.set_xlabel(\"\")\n",
        "            ax.set_ylabel(metric, fontsize=12)\n",
        "            ax.text(0.5, 1.02, label, transform=ax.transAxes, ha=\"center\", va=\"bottom\")\n",
        "\n",
        "            sns.despine(ax=ax)\n",
        "\n",
        "        plt.show()\n",
        "        _last_fig = fig\n",
        "\n",
        "def save_plots(_=None):\n",
        "    if _last_fig is None:\n",
        "        with out:\n",
        "            print(\"Nothing to save yet.\")\n",
        "        return\n",
        "    fname = f\"metrics_grid_{int(time.time())}.pdf\"\n",
        "    _last_fig.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
        "    with out:\n",
        "        print(f\"Saved {fname}\")\n",
        "    if colab_files is not None:\n",
        "        colab_files.download(fname)\n",
        "\n",
        "save_btn.on_click(save_plots)\n",
        "\n",
        "display(save_btn)\n",
        "display(out)\n",
        "\n",
        "run_plots()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yHqwPdFOw1Rz"
      },
      "id": "yHqwPdFOw1Rz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RL fitting functions\n",
        "\n",
        "def extract_context_action_reward(df):\n",
        "\tnum_events = len(df)\n",
        "\n",
        "\tnum_trials = 0 # the number of valid trials\n",
        "\tfor event in df['Event']:\n",
        "\t\tif event == 'Left' or event == 'Right':\n",
        "\t\t\tnum_trials += 1\n",
        "\n",
        "\tsimple_data = {'context': np.zeros((num_trials)), 'action': np.zeros((num_trials)), 'reward': np.zeros((num_trials))}\n",
        "\ttrial_idx = 0\n",
        "\tfor eidx, event in enumerate(df['Event']):\n",
        "\t\tif event == 'Left' or event == 'Right':\n",
        "\t\t\t# action\n",
        "\t\t\tif event == 'Right':\n",
        "\t\t\t\tsimple_data['action'][trial_idx] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tsimple_data['action'][trial_idx] = 0\n",
        "\t\t\t# reward\n",
        "\t\t\tif eidx + 1 < len(df) and df['Event'].iloc[eidx + 1] == 'Pellet':\n",
        "\t\t\t\tsimple_data['reward'][trial_idx] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tsimple_data['reward'][trial_idx] = 0\n",
        "\t\t\t# context\n",
        "\t\t\tif df['High_prob_poke'].iloc[eidx] == 'Right':\n",
        "\t\t\t\tsimple_data['context'][trial_idx] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tsimple_data['context'][trial_idx] = 0\n",
        "\t\t\t#\n",
        "\t\t\ttrial_idx += 1\n",
        "\n",
        "\treturn simple_data\n",
        "\n",
        "\n",
        "# get basic stats from [action, reward, context] sequence\n",
        "def get_basic_stats(session_data):\n",
        "\tactions = session_data['action']\n",
        "\trewards = session_data['reward']\n",
        "\tcontexts = session_data['context']\n",
        "\n",
        "\ts_stats = {}\n",
        "\tsession_length = len(actions)\n",
        "\tif session_length == 0:\n",
        "\t\treturn dict(session_length=0, total_rewards=0, reward_rate=np.nan,\n",
        "\t\t\t\tcorrect_port_rate=np.nan, action_bias=np.nan)\n",
        "\telse:\n",
        "\t\ts_stats['session_length'] = session_length\n",
        "\t\ts_stats['total_rewards'] = np.sum(rewards)\n",
        "\t\ts_stats['reward_rate'] = s_stats['total_rewards']/session_length\n",
        "\t\ts_stats['correct_port_rate'] = np.sum( actions==contexts )/session_length\n",
        "\t\ts_stats['action_bias'] = 2*( np.sum(actions)/session_length - 0.5 )\n",
        "\n",
        "\t\treturn s_stats\n",
        "\n",
        "\n",
        "  # Model fitting\n",
        "# -------------------------\n",
        "# Model & loss (JAX-native)\n",
        "# -------------------------\n",
        "def _one_step(carry, inp):\n",
        "    qL, qR, alpha, beta, bias, lapse, c_preva, prev_action, eps = carry\n",
        "    action, reward = inp\n",
        "    signed_prev_action = 2 * (prev_action - 1/2)\n",
        "\n",
        "    logit = beta * (qR - qL + c_preva * signed_prev_action + bias)\n",
        "    p_right = 0.5 * lapse + (1.0 - lapse) * jax.nn.sigmoid(logit)\n",
        "    p_right = jnp.clip(p_right, eps, 1.0 - eps)\n",
        "\n",
        "    loss_t = -(action * jnp.log(p_right) + (1 - action) * jnp.log1p(-p_right))\n",
        "\n",
        "    qR_new = qR + action * alpha * (reward - qR)\n",
        "    qL_new = qL + (1 - action) * alpha * (reward - qL)\n",
        "    return (qL_new, qR_new, alpha, beta, bias, lapse, c_preva, action, eps), (p_right, loss_t)\n",
        "\n",
        "\n",
        "def calc_loss(alpha, beta, bias, lapse, c_preva, actions, rewards):\n",
        "    eps = 1e-8\n",
        "    prev_action = 0.5\n",
        "    carry0 = (jnp.array(0.0), jnp.array(0.0), alpha, beta, bias, lapse, c_preva, prev_action, eps)\n",
        "    _, outputs = lax.scan(_one_step, carry0, (actions, rewards))\n",
        "    losses = outputs[1]\n",
        "    return jnp.sum(losses)\n",
        "\n",
        "\n",
        "def loss_fn(params, actions, rewards):\n",
        "    return calc_loss(params['alpha'], params['beta'], params['bias'], params['lapse'], params['c_preva'],\n",
        "                     actions, rewards)\n",
        "\n",
        "# -----------------------------\n",
        "# Training with Adam optimizer\n",
        "# -----------------------------\n",
        "def fit_with_adam(data,\n",
        "                  lr=1e-2,\n",
        "                  num_iterations=1000,\n",
        "                  param_ranges=None,\n",
        "                  seed=0,\n",
        "                  RL_model='preva'): # Add RL_model parameter\n",
        "    \"\"\"\n",
        "    Trains alpha, beta, bias, lapse using Adam on session NLL.\n",
        "    \"\"\"\n",
        "    if param_ranges is None:\n",
        "        param_ranges = {\n",
        "            'alpha': (0.0, 1.0),\n",
        "            'beta':  (0.0, 20.0),\n",
        "            'bias':  (-1.0, 1.0),\n",
        "            'lapse': (0.0, 1.0),\n",
        "            'c_preva': (0.0, 1.0),\n",
        "        }\n",
        "\n",
        "    # Adjust param_ranges for c_preva if RL_model is 'vanilla'\n",
        "    if RL_model == 'vanilla':\n",
        "        param_ranges['c_preva'] = (0.0, 0.0) # Fix c_preva to 0\n",
        "\n",
        "    actions = jnp.asarray(data['action'])\n",
        "    rewards = jnp.asarray(data['reward'])\n",
        "    # contexts = jnp.asarray(data['context'])  # unused in current model\n",
        "\n",
        "    # init params\n",
        "    rng = np.random.default_rng(seed)\n",
        "    params = {\n",
        "        k: jnp.array(rng.uniform(low=lo, high=hi))\n",
        "        for k, (lo, hi) in param_ranges.items()\n",
        "    }\n",
        "\n",
        "    # Ensure c_preva is 0 if RL_model is 'vanilla', even if random init was non-zero\n",
        "    if RL_model == 'vanilla':\n",
        "        params['c_preva'] = jnp.array(0.0)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optax.adam(lr)\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    # projection to bounds\n",
        "    def project(p):\n",
        "        return {k: jnp.clip(v, *param_ranges[k]) for k, v in p.items()}\n",
        "\n",
        "    @jax.jit\n",
        "    def step(params, opt_state, actions, rewards):\n",
        "        loss, grads = jax.value_and_grad(loss_fn)(params, actions, rewards)\n",
        "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "        params = optax.apply_updates(params, updates)\n",
        "        params = project(params)  # keep within bounds\n",
        "        return params, opt_state, loss\n",
        "\n",
        "    losses = []\n",
        "    p = params\n",
        "    s = opt_state\n",
        "    for _ in range(num_iterations): # iterative update of parameters\n",
        "        p, s, loss = step(p, s, actions, rewards)\n",
        "        losses.append(float(loss))\n",
        "\n",
        "    # return Python floats for convenience\n",
        "    learned = {k: float(v) for k, v in p.items()}\n",
        "    return learned, np.asarray(losses)\n",
        "\n",
        "## meta data incorporation\n",
        "\n",
        "files_list = feds_cropped if 'feds_cropped' in globals() else feds\n",
        "assert isinstance(files_list, list) and len(files_list) > 0, \"No FED3 files loaded.\"\n",
        "assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "\n",
        "# metadata_df = copy of Key_Df\n",
        "metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "if 'filename' in metadata_df.columns:\n",
        "    metadata_df['filename'] = metadata_df['filename'].astype(str).map(os.path.basename)\n",
        "\n",
        "def _coerce_numeric_col(df, col, clip_upper=None, na_map=None):\n",
        "    if col not in df.columns:\n",
        "        return\n",
        "    s = df[col]\n",
        "    if na_map:\n",
        "        s = s.replace(na_map)\n",
        "    s = pd.to_numeric(s, errors='coerce')\n",
        "    if clip_upper is not None:\n",
        "        s.loc[s > clip_upper] = np.nan\n",
        "    df[col] = s\n",
        "\n",
        "def attach_meta_data(file_index):\n",
        "    df = files_list[file_index].copy()\n",
        "    full_name = getattr(df, 'name', f\"File_{file_index}\")\n",
        "    file_basename = os.path.basename(str(full_name))\n",
        "\n",
        "    # Preserve original index once\n",
        "    if \"Original_Timestamp\" not in df.columns:\n",
        "        df[\"Original_Timestamp\"] = df.index\n",
        "\n",
        "    # Attach metadata by filename (matching already done upstream)\n",
        "    meta_row = None\n",
        "    if 'filename' in metadata_df.columns:\n",
        "        mr = metadata_df.loc[metadata_df['filename'] == file_basename]\n",
        "        if not mr.empty:\n",
        "            meta_row = mr.iloc[0]\n",
        "    if meta_row is not None:\n",
        "        for col in meta_row.index:\n",
        "            if col == 'filename':\n",
        "                continue\n",
        "            if col not in df.columns:\n",
        "                df[col] = meta_row[col]\n",
        "            else:\n",
        "                if pd.isna(df[col]).all() and pd.notna(meta_row[col]):\n",
        "                    df[col] = meta_row[col]\n",
        "\n",
        "    # Time + cleanup\n",
        "    try:\n",
        "        df['timestamp'] = pd.to_datetime(df.index)\n",
        "    except Exception:\n",
        "        df['timestamp'] = np.arange(len(df))\n",
        "\n",
        "    _coerce_numeric_col(df, 'Poke_Time', clip_upper=2)\n",
        "    _coerce_numeric_col(df, 'Retrieval_Time', na_map={\"Timed_out\": np.nan})\n",
        "\n",
        "    if len(df) == 0:\n",
        "        print(f\"[!] Empty dataframe for {file_basename}. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    return df\n",
        "\n",
        "df = attach_meta_data(1)\n"
      ],
      "metadata": {
        "id": "cX69vpLieKEO",
        "cellView": "form"
      },
      "id": "cX69vpLieKEO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RL fitting (this may take ~30-60 minutes)\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# inferring the RL parameters (alpha, beta, bias, lapse, c_preva)\n",
        "# alpha : learning rate parameter\n",
        "# beta : inverse temperature\n",
        "# bias : bias in the action\n",
        "# lapse:\n",
        "# c_preva : the coefficient of action history term\n",
        "#\n",
        "# vanilla: vanilla Rescorla-Wagner model\n",
        "# preva : Rescorla-Wagner model with previous-action variable for capturing hysteresis factor\n",
        "#\n",
        "def infer_params( session_data, hy_params, if_plot=False ):\n",
        "\tlr = hy_params['lr']\n",
        "\tnum_iterations = hy_params['num_iterations']\n",
        "\tfitting_reps = hy_params['fitting_reps']\n",
        "\tRL_model = hy_params['RL_model'] # Get RL_model from hy_params\n",
        "\n",
        "\tfor i in range(fitting_reps):\n",
        "\t\tfitted_params, losses = fit_with_adam(session_data, lr, num_iterations, seed=rng.choice(range(1000000)), RL_model=RL_model ) # Pass RL_model\n",
        "\t\tif i == 0:\n",
        "\t\t\tbest_params, best_loss = fitted_params, losses[-1]\n",
        "\t\telif losses[-1] < best_loss:\n",
        "\t\t\tbest_params = fitted_params\n",
        "\t\t\tbest_loss = losses[-1]\n",
        "\t\tprint(i, losses[-1]) #fitted_params_emsemble.append( fitted_params )\n",
        "\t\tif if_plot:\n",
        "\t\t\tplt.plot(losses)\n",
        "\tif if_plot:\n",
        "\t\tplt.xlabel('fitting iterations')\n",
        "\t\tplt.ylabel('fitting error')\n",
        "\t\tplt.show()\n",
        "\n",
        "\treturn best_params, best_loss\n",
        "\n",
        "def run_fitting(fitting_repetition = 10):\n",
        "  hy_params = {\n",
        "      'fitting_reps': fitting_repetition, # the number of fitting repetitions (from different initial values). More is better, but 10 seems to be enough\n",
        "      'lr': 5e-3, # learning rate (of Adam optimization)\n",
        "      'num_iterations': 1000, # the total number of iteration per fitting\n",
        "      'RL_model': 'preva', # {'vanilla', 'preva'};  Model choice\n",
        "      }\n",
        "\n",
        "  session_df = attach_meta_data(0)\n",
        "  hy_params['task'] = session_df['Session_Type'].iloc[0]\n",
        "\n",
        "  gene_name_id = str( session_df['Gene'].iloc[0] ) + '_'\n",
        "  festr = 'pdata/RW_fitting_gene_' + gene_name_id + '_task_' + hy_params['task'] + '_model_' + hy_params['RL_model']\\\n",
        "          + '_freps' + str(hy_params['fitting_reps']) + '_lr' + str(hy_params['lr']) + '_niters' + str(hy_params['num_iterations']) + '.csv'\n",
        "  os.makedirs(os.path.dirname(festr), exist_ok=True)\n",
        "  few = open(festr,'w')\n",
        "\n",
        "  RL_df = pd.DataFrame() # Initialize RL_df here\n",
        "  for sidx in tqdm(range( len(files_list) )): # do fitting for all files in the files_list\n",
        "    session_df = attach_meta_data(sidx)\n",
        "    session_data = extract_context_action_reward(session_df)\n",
        "    s_stats = get_basic_stats(session_data)\n",
        "    best_params, best_loss = infer_params( session_data, hy_params, if_plot=False )\n",
        "\n",
        "    new_row = (s_stats | best_params) if hasattr(dict, '__or__') else {**s_stats, **best_params}\n",
        "    new_row[\"best_loss\"] = best_loss / s_stats[\"session_length\"]\n",
        "    for key in ['Mouse_ID', 'Gene', 'Sex']: # add the basic meta data\n",
        "      new_row[key] = session_df[key].iloc[0]\n",
        "\n",
        "    if sidx == 0:\n",
        "      RL_df = pd.DataFrame(columns=new_row.keys())\n",
        "    RL_df.loc[len(RL_df)] = new_row\n",
        "\n",
        "    # write down the best parameter into a file\n",
        "    ltmps = str(s_stats['session_length']) + ',' + str(s_stats['reward_rate']) + ','+ str(s_stats['correct_port_rate'])\\\n",
        "            + ',' + str(s_stats['action_bias']) + ','  + str(best_params['alpha']) + ',' + str(best_params['beta'])\\\n",
        "            + ',' + str(best_params['bias']) + ',' + str(best_params['lapse']) + ',' + str(best_params['c_preva']) + ','\\\n",
        "            + str(best_loss/s_stats['session_length'])\n",
        "    few.write(ltmps + '\\n')\n",
        "  few.flush()\n",
        "  return RL_df\n",
        "\n",
        "RL_df = run_fitting(fitting_repetition=10)\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "JPEMVoVnhc7O"
      },
      "id": "JPEMVoVnhc7O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - make this plot look like the others :)\n",
        "\n",
        "# @title Plot the RL metrics\n",
        "plt.figure(figsize=(12, 3))\n",
        "\n",
        "for kyidx, key in enumerate( ['alpha', 'beta', 'bias', 'lapse', 'c_preva', 'best_loss'] ):\n",
        "  plt.subplot(1,6,kyidx+1)\n",
        "  sns.boxplot(data=RL_df, x='Sex', y=key)\n",
        "#  sns.swarmplot(data=RL_df, x='Genotype', y=key)\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-j721NsS2oAm"
      },
      "id": "-j721NsS2oAm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download the RL metrics\n",
        "\n",
        "\n",
        "rl_df = RL_df.copy()\n",
        "\n",
        "#rl_df[\"Session_type\"] = rl_df[\"Session_type\"] + \"_RL\"\n",
        "\n",
        "output_file = f\"RL_modeling.csv\"\n",
        "\n",
        "\n",
        "RL_df.to_csv(output_file, index=False)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(output_file)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(output_file)}</code>…\"\n",
        "        gfiles.download(output_file)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{output_file}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iKAshcHmhx18"
      },
      "id": "iKAshcHmhx18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LzmxH-ZBSTB"
      },
      "id": "-LzmxH-ZBSTB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (Spyder)",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}